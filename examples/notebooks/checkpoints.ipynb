{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-14T10:09:03.170226Z",
     "start_time": "2025-12-14T09:43:49.396017Z"
    }
   },
   "source": [
    "# train the model\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from softgrad import Network\n",
    "from softgrad.Checkpoint import Checkpoint\n",
    "from softgrad.layer.reshape import Flatten\n",
    "from softgrad.layer.shim import MLX\n",
    "from softgrad.optim import SGD\n",
    "from softgrad.function.activation import leaky_relu, softmax\n",
    "from softgrad.function.loss import CrossEntropyLoss, cross_entropy_loss\n",
    "from softgrad.layer.core import Linear, Activation\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from util.dataset import get_cifar10\n",
    "\n",
    "\n",
    "# Visualize\n",
    "def viz_sample_predictions(network, test_data, label_map, rows=5, cols=5, figsize=(10, 10)):\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize, num=\"Sample Predictions\")\n",
    "    axes = axes.reshape(-1)  # flatten\n",
    "\n",
    "    test_data = test_data.to_buffer().shuffle()\n",
    "    def sample_random():\n",
    "        for j in np.arange(0, rows * cols):\n",
    "            i = np.random.randint(0, len(test_data))\n",
    "            x = mx.array(test_data[i]['image'])\n",
    "            y = mx.array(test_data[i]['label'])\n",
    "            y_pred = network.forward(x[mx.newaxis, ...])\n",
    "\n",
    "            sample = np.array(255 * x)\n",
    "            if sample.shape[2] == 3:\n",
    "                image = Image.fromarray(sample.astype('uint8'))\n",
    "            else:\n",
    "                image = Image.fromarray(sample.reshape(sample.shape[0], sample.shape[1]))\n",
    "\n",
    "            raw_label = mx.argmax(y).item()\n",
    "            label = label_map[raw_label]\n",
    "\n",
    "            raw_pred = mx.argmax(y_pred).item()\n",
    "            pred = label_map[raw_pred]\n",
    "\n",
    "            axes[j].imshow(image)\n",
    "            axes[j].set_title(f\"True: {label} \\nPredict: {pred}\")\n",
    "            axes[j].axis('off')\n",
    "            plt.subplots_adjust(wspace=1)\n",
    "\n",
    "    def on_key(event):\n",
    "        if event.key == ' ':\n",
    "            sample_random()\n",
    "            fig.show()\n",
    "\n",
    "    fig.canvas.mpl_connect('key_press_event', on_key)\n",
    "\n",
    "    sample_random()\n",
    "\n",
    "\n",
    "def viz_history(history, figsize=(6, 4)):\n",
    "    plt.figure(figsize=figsize, num=\"Loss Curves\")\n",
    "    plt.plot(history['epoch'], history['train_loss'], 'black', linewidth=2.0)\n",
    "    plt.plot(history['epoch'], history['test_loss'], 'green', linewidth=2.0)\n",
    "    plt.legend(['Training Loss', 'Validation Loss'], fontsize=14)\n",
    "    plt.xlabel('Epochs', fontsize=10)\n",
    "    plt.ylabel('Loss', fontsize=10)\n",
    "    plt.title('Loss vs Epoch', fontsize=12)\n",
    "\n",
    "    plt.figure(figsize=figsize, num=\"Accuracy Curves\")\n",
    "    plt.plot(history['epoch'], history['train_accuracy'], 'black', linewidth=2.0)\n",
    "    plt.plot(history['epoch'], history['test_accuracy'], 'green', linewidth=2.0)\n",
    "    plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=14)\n",
    "    plt.xlabel('Epochs', fontsize=10)\n",
    "    plt.ylabel('Accuracy', fontsize=10)\n",
    "    plt.title('Accuracy vs Epoch', fontsize=12)\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "def eval_model(model, dataset, epoch=None):\n",
    "    mean_losses = []\n",
    "    accuracies = []\n",
    "    predictions = []\n",
    "\n",
    "    for batch in dataset:\n",
    "        x_batch = mx.array(batch[\"image\"])\n",
    "        y_batch = mx.array(batch[\"label\"])\n",
    "\n",
    "        y_pred = model.forward(x_batch)\n",
    "        predictions.append(y_pred)\n",
    "\n",
    "        loss = optimizer.loss_fn(y_pred, y_batch)\n",
    "        mean_loss = mx.mean(mx.sum(loss, axis=1))\n",
    "        mean_losses.append(mean_loss.item())\n",
    "\n",
    "        if isinstance(optimizer.loss_fn, CrossEntropyLoss):\n",
    "            y_pred = softmax(y_pred)\n",
    "\n",
    "        errors = mx.sum(mx.abs(y_batch - mx.round(y_pred)), axis=1)\n",
    "        accuracy = mx.sum(errors == 0) / y_batch.shape[0]\n",
    "        accuracies.append(accuracy.item())\n",
    "\n",
    "    mean_loss = sum(mean_losses) / len(mean_losses)\n",
    "    accuracy = sum(accuracies) / len(accuracies)\n",
    "    predictions = np.concatenate(predictions)\n",
    "\n",
    "    dataset.reset()\n",
    "\n",
    "    if epoch is not None:\n",
    "        print(f\"Epoch {epoch}: Accuracy {accuracy:.3f}, Average Loss {mean_loss}\")\n",
    "    else:\n",
    "        print(f\"Accuracy {accuracy:.3f}, Average Loss {mean_loss}\")\n",
    "        print(f\"Accuracy {accuracy:.3f}, Average Loss {mean_loss}\")\n",
    "\n",
    "    return predictions, accuracy, mean_loss\n",
    "\n",
    "\n",
    "def train(train_data, epochs, batch_size=1, test_data=None, cb=None):\n",
    "    batched_train_data = train_data.batch(batch_size)\n",
    "    batched_test_data = test_data.batch(batch_size)\n",
    "\n",
    "    def train_epoch():\n",
    "        for batch in batched_train_data:\n",
    "            x_batch = mx.array(batch[\"image\"])\n",
    "            y_batch = mx.array(batch[\"label\"])\n",
    "            optimizer.step(x_batch, y_batch)\n",
    "        batched_train_data.reset()\n",
    "\n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"test_loss\": [], \"train_accuracy\": [], \"test_accuracy\": []}\n",
    "\n",
    "    _, train_accuracy, train_loss = eval_model(network, batched_train_data, epoch=0)\n",
    "    _, test_accuracy, test_loss = eval_model(network, batched_test_data, epoch=0)\n",
    "    print()\n",
    "    history[\"epoch\"].append(0)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"test_loss\"].append(test_loss)\n",
    "    history[\"train_accuracy\"].append(train_accuracy)\n",
    "    history[\"test_accuracy\"].append(test_accuracy)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_epoch()\n",
    "\n",
    "        _, train_accuracy, train_loss = eval_model(network, batched_train_data, epoch=epoch)\n",
    "        _, test_accuracy, test_loss = eval_model(network, batched_test_data, epoch=epoch)\n",
    "        print()\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        history[\"train_accuracy\"].append(train_accuracy)\n",
    "        history[\"test_accuracy\"].append(test_accuracy)\n",
    "\n",
    "    test_data.reset()\n",
    "    eval_model(network, batched_test_data)\n",
    "    print()\n",
    "\n",
    "    viz_sample_predictions(network, test_data, label_map)\n",
    "    viz_history(history)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "train_data, test_data = get_cifar10(static=False)\n",
    "label_map = [\"Airplane\", \"Automobile\", \"Bird\", \"Cat\", \"Deer\", \"Dog\", \"Frog\", \"Horse\", \"Ship\", \"Truck\"]\n",
    "\n",
    "network = Network(input_shape=(32, 32, 3))\n",
    "\n",
    "# conv block 1\n",
    "network.add_layer(MLX(nn.Conv2d(in_channels=3, out_channels=96, kernel_size=7)))\n",
    "network.add_layer(Activation(leaky_relu))\n",
    "network.add_layer(MLX(nn.MaxPool2d(2)))\n",
    "# conv block 2\n",
    "network.add_layer(MLX(nn.Conv2d(in_channels=96, out_channels=256, kernel_size=3)))\n",
    "network.add_layer(Activation(leaky_relu))\n",
    "network.add_layer(MLX(nn.MaxPool2d(2)))\n",
    "# feed forward\n",
    "network.add_layer(Flatten())\n",
    "network.add_layer(Linear(1024))\n",
    "network.add_layer(Activation(leaky_relu))\n",
    "network.add_layer(Linear(10))\n",
    "\n",
    "network.load(Checkpoint.read(\"checkpoints/simple_conv.pb\"))\n",
    "\n",
    "optimizer = SGD(eta=0.05, momentum=0.9, weight_decay=0.0005)\n",
    "optimizer.bind_loss_fn(cross_entropy_loss)\n",
    "optimizer.bind_network(network)\n",
    "\n",
    "train(train_data, epochs=200, batch_size=1000, test_data=test_data)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Accuracy 0.990, Average Loss 0.027245118170976638\n",
      "Epoch 0: Accuracy 0.814, Average Loss 1.2314345002174378\n",
      "\n",
      "Epoch 1: Accuracy 0.992, Average Loss 0.024384953919798134\n",
      "Epoch 1: Accuracy 0.811, Average Loss 1.2623634099960328\n",
      "\n",
      "Epoch 2: Accuracy 0.992, Average Loss 0.023170945011079313\n",
      "Epoch 2: Accuracy 0.817, Average Loss 1.2387227058410644\n",
      "\n",
      "Epoch 3: Accuracy 0.990, Average Loss 0.026110394336283208\n",
      "Epoch 3: Accuracy 0.812, Average Loss 1.2660954594612122\n",
      "\n",
      "Epoch 4: Accuracy 0.990, Average Loss 0.02784379171207547\n",
      "Epoch 4: Accuracy 0.812, Average Loss 1.2651890993118287\n",
      "\n",
      "Epoch 5: Accuracy 0.992, Average Loss 0.023671937696635722\n",
      "Epoch 5: Accuracy 0.812, Average Loss 1.2621628999710084\n",
      "\n",
      "Epoch 6: Accuracy 0.988, Average Loss 0.033738438114523886\n",
      "Epoch 6: Accuracy 0.810, Average Loss 1.275539791584015\n",
      "\n",
      "Epoch 7: Accuracy 0.990, Average Loss 0.028240051809698342\n",
      "Epoch 7: Accuracy 0.816, Average Loss 1.2840267419815063\n",
      "\n",
      "Epoch 8: Accuracy 0.992, Average Loss 0.023699729330837728\n",
      "Epoch 8: Accuracy 0.817, Average Loss 1.2705386877059937\n",
      "\n",
      "Epoch 9: Accuracy 0.991, Average Loss 0.025799765679985286\n",
      "Epoch 9: Accuracy 0.815, Average Loss 1.2850956439971923\n",
      "\n",
      "Epoch 10: Accuracy 0.992, Average Loss 0.024163881316781044\n",
      "Epoch 10: Accuracy 0.815, Average Loss 1.2897826313972474\n",
      "\n",
      "Epoch 11: Accuracy 0.992, Average Loss 0.022964167278259992\n",
      "Epoch 11: Accuracy 0.816, Average Loss 1.263550627231598\n",
      "\n",
      "Epoch 12: Accuracy 0.991, Average Loss 0.027702377736568452\n",
      "Epoch 12: Accuracy 0.810, Average Loss 1.2697401165962219\n",
      "\n",
      "Epoch 13: Accuracy 0.989, Average Loss 0.03142968399450183\n",
      "Epoch 13: Accuracy 0.812, Average Loss 1.2958775401115417\n",
      "\n",
      "Epoch 14: Accuracy 0.992, Average Loss 0.02256209496408701\n",
      "Epoch 14: Accuracy 0.813, Average Loss 1.268217718601227\n",
      "\n",
      "Epoch 15: Accuracy 0.992, Average Loss 0.022863091360777618\n",
      "Epoch 15: Accuracy 0.817, Average Loss 1.271226179599762\n",
      "\n",
      "Epoch 16: Accuracy 0.990, Average Loss 0.028897299654781817\n",
      "Epoch 16: Accuracy 0.811, Average Loss 1.2845714211463928\n",
      "\n",
      "Epoch 17: Accuracy 0.992, Average Loss 0.02306040208786726\n",
      "Epoch 17: Accuracy 0.816, Average Loss 1.2776248455047607\n",
      "\n",
      "Epoch 18: Accuracy 0.990, Average Loss 0.028542238865047694\n",
      "Epoch 18: Accuracy 0.814, Average Loss 1.2806697130203246\n",
      "\n",
      "Epoch 19: Accuracy 0.992, Average Loss 0.023602857124060392\n",
      "Epoch 19: Accuracy 0.815, Average Loss 1.2451831340789794\n",
      "\n",
      "Epoch 20: Accuracy 0.993, Average Loss 0.01936807133257389\n",
      "Epoch 20: Accuracy 0.816, Average Loss 1.2682423353195191\n",
      "\n",
      "Epoch 21: Accuracy 0.992, Average Loss 0.025513772908598183\n",
      "Epoch 21: Accuracy 0.813, Average Loss 1.2590695142745971\n",
      "\n",
      "Epoch 22: Accuracy 0.990, Average Loss 0.027396154180169104\n",
      "Epoch 22: Accuracy 0.811, Average Loss 1.2890014886856078\n",
      "\n",
      "Epoch 23: Accuracy 0.993, Average Loss 0.021155279241502286\n",
      "Epoch 23: Accuracy 0.814, Average Loss 1.27029789686203\n",
      "\n",
      "Epoch 24: Accuracy 0.991, Average Loss 0.024897371120750904\n",
      "Epoch 24: Accuracy 0.812, Average Loss 1.3038705587387085\n",
      "\n",
      "Epoch 25: Accuracy 0.993, Average Loss 0.020875056106597185\n",
      "Epoch 25: Accuracy 0.817, Average Loss 1.2765246868133544\n",
      "\n",
      "Epoch 26: Accuracy 0.993, Average Loss 0.019783870372921227\n",
      "Epoch 26: Accuracy 0.814, Average Loss 1.2730031251907348\n",
      "\n",
      "Epoch 27: Accuracy 0.991, Average Loss 0.02551464356482029\n",
      "Epoch 27: Accuracy 0.813, Average Loss 1.285770559310913\n",
      "\n",
      "Epoch 28: Accuracy 0.992, Average Loss 0.022630191706120968\n",
      "Epoch 28: Accuracy 0.817, Average Loss 1.2763391613960267\n",
      "\n",
      "Epoch 29: Accuracy 0.993, Average Loss 0.01954685792326927\n",
      "Epoch 29: Accuracy 0.814, Average Loss 1.2730181694030762\n",
      "\n",
      "Epoch 30: Accuracy 0.992, Average Loss 0.02301558993756771\n",
      "Epoch 30: Accuracy 0.814, Average Loss 1.2909886479377746\n",
      "\n",
      "Epoch 31: Accuracy 0.992, Average Loss 0.02360514134168625\n",
      "Epoch 31: Accuracy 0.814, Average Loss 1.3034481287002564\n",
      "\n",
      "Epoch 32: Accuracy 0.992, Average Loss 0.020485789338126777\n",
      "Epoch 32: Accuracy 0.814, Average Loss 1.283192265033722\n",
      "\n",
      "Epoch 33: Accuracy 0.993, Average Loss 0.02118793087080121\n",
      "Epoch 33: Accuracy 0.814, Average Loss 1.2687472343444823\n",
      "\n",
      "Epoch 34: Accuracy 0.992, Average Loss 0.02269450206309557\n",
      "Epoch 34: Accuracy 0.817, Average Loss 1.286756432056427\n",
      "\n",
      "Epoch 35: Accuracy 0.992, Average Loss 0.024109333362430332\n",
      "Epoch 35: Accuracy 0.811, Average Loss 1.309444212913513\n",
      "\n",
      "Epoch 36: Accuracy 0.993, Average Loss 0.01937851483002305\n",
      "Epoch 36: Accuracy 0.815, Average Loss 1.2811419367790222\n",
      "\n",
      "Epoch 37: Accuracy 0.991, Average Loss 0.024516211096197366\n",
      "Epoch 37: Accuracy 0.815, Average Loss 1.2954733610153197\n",
      "\n",
      "Epoch 38: Accuracy 0.993, Average Loss 0.018922426011413335\n",
      "Epoch 38: Accuracy 0.815, Average Loss 1.308334505558014\n",
      "\n",
      "Epoch 39: Accuracy 0.992, Average Loss 0.025236665308475494\n",
      "Epoch 39: Accuracy 0.819, Average Loss 1.2967015147209167\n",
      "\n",
      "Epoch 40: Accuracy 0.992, Average Loss 0.023476575668901205\n",
      "Epoch 40: Accuracy 0.816, Average Loss 1.3001091957092286\n",
      "\n",
      "Epoch 41: Accuracy 0.993, Average Loss 0.020411470234394075\n",
      "Epoch 41: Accuracy 0.817, Average Loss 1.2840877175331116\n",
      "\n",
      "Epoch 42: Accuracy 0.992, Average Loss 0.020193592924624682\n",
      "Epoch 42: Accuracy 0.816, Average Loss 1.2764836549758911\n",
      "\n",
      "Epoch 43: Accuracy 0.992, Average Loss 0.022251529498025776\n",
      "Epoch 43: Accuracy 0.815, Average Loss 1.3146887302398682\n",
      "\n",
      "Epoch 44: Accuracy 0.993, Average Loss 0.020473700910806657\n",
      "Epoch 44: Accuracy 0.816, Average Loss 1.2902286291122436\n",
      "\n",
      "Epoch 45: Accuracy 0.993, Average Loss 0.01950934836640954\n",
      "Epoch 45: Accuracy 0.816, Average Loss 1.2620816946029663\n",
      "\n",
      "Epoch 46: Accuracy 0.993, Average Loss 0.020098450975492597\n",
      "Epoch 46: Accuracy 0.818, Average Loss 1.277739679813385\n",
      "\n",
      "Epoch 47: Accuracy 0.993, Average Loss 0.019293750859797\n",
      "Epoch 47: Accuracy 0.819, Average Loss 1.2657556176185607\n",
      "\n",
      "Epoch 48: Accuracy 0.994, Average Loss 0.017914008563384414\n",
      "Epoch 48: Accuracy 0.817, Average Loss 1.2676783919334411\n",
      "\n",
      "Epoch 49: Accuracy 0.993, Average Loss 0.020897570829838515\n",
      "Epoch 49: Accuracy 0.814, Average Loss 1.2955908179283142\n",
      "\n",
      "Epoch 50: Accuracy 0.993, Average Loss 0.018904211968183517\n",
      "Epoch 50: Accuracy 0.815, Average Loss 1.282988476753235\n",
      "\n",
      "Epoch 51: Accuracy 0.993, Average Loss 0.020012658163905142\n",
      "Epoch 51: Accuracy 0.811, Average Loss 1.3184961915016173\n",
      "\n",
      "Epoch 52: Accuracy 0.992, Average Loss 0.021930088615044953\n",
      "Epoch 52: Accuracy 0.813, Average Loss 1.3026388883590698\n",
      "\n",
      "Epoch 53: Accuracy 0.992, Average Loss 0.023977828156203033\n",
      "Epoch 53: Accuracy 0.815, Average Loss 1.2928320646286011\n",
      "\n",
      "Epoch 54: Accuracy 0.992, Average Loss 0.022024666350334884\n",
      "Epoch 54: Accuracy 0.816, Average Loss 1.3336191296577453\n",
      "\n",
      "Epoch 55: Accuracy 0.993, Average Loss 0.01856558796018362\n",
      "Epoch 55: Accuracy 0.813, Average Loss 1.2823238968849182\n",
      "\n",
      "Epoch 56: Accuracy 0.993, Average Loss 0.018937939908355475\n",
      "Epoch 56: Accuracy 0.820, Average Loss 1.305048406124115\n",
      "\n",
      "Epoch 57: Accuracy 0.992, Average Loss 0.02277369109913707\n",
      "Epoch 57: Accuracy 0.814, Average Loss 1.307179033756256\n",
      "\n",
      "Epoch 58: Accuracy 0.993, Average Loss 0.02056491931900382\n",
      "Epoch 58: Accuracy 0.819, Average Loss 1.300968337059021\n",
      "\n",
      "Epoch 59: Accuracy 0.993, Average Loss 0.01851221889257431\n",
      "Epoch 59: Accuracy 0.816, Average Loss 1.304260015487671\n",
      "\n",
      "Epoch 60: Accuracy 0.993, Average Loss 0.019821251593530178\n",
      "Epoch 60: Accuracy 0.815, Average Loss 1.2817145705223083\n",
      "\n",
      "Epoch 61: Accuracy 0.993, Average Loss 0.020959504935890435\n",
      "Epoch 61: Accuracy 0.817, Average Loss 1.2954450488090514\n",
      "\n",
      "Epoch 62: Accuracy 0.993, Average Loss 0.021742300204932688\n",
      "Epoch 62: Accuracy 0.817, Average Loss 1.3066609263420106\n",
      "\n",
      "Epoch 63: Accuracy 0.993, Average Loss 0.020379371643066406\n",
      "Epoch 63: Accuracy 0.822, Average Loss 1.289751410484314\n",
      "\n",
      "Epoch 64: Accuracy 0.993, Average Loss 0.021449553770944475\n",
      "Epoch 64: Accuracy 0.815, Average Loss 1.3038624048233032\n",
      "\n",
      "Epoch 65: Accuracy 0.993, Average Loss 0.01935330657288432\n",
      "Epoch 65: Accuracy 0.817, Average Loss 1.2945013642311096\n",
      "\n",
      "Epoch 66: Accuracy 0.992, Average Loss 0.02313725434243679\n",
      "Epoch 66: Accuracy 0.814, Average Loss 1.3192325234413147\n",
      "\n",
      "Epoch 67: Accuracy 0.991, Average Loss 0.026055625136941672\n",
      "Epoch 67: Accuracy 0.814, Average Loss 1.3361406326293945\n",
      "\n",
      "Epoch 68: Accuracy 0.994, Average Loss 0.01758898962289095\n",
      "Epoch 68: Accuracy 0.818, Average Loss 1.2652649521827697\n",
      "\n",
      "Epoch 69: Accuracy 0.990, Average Loss 0.027390708327293397\n",
      "Epoch 69: Accuracy 0.814, Average Loss 1.3211404442787171\n",
      "\n",
      "Epoch 70: Accuracy 0.994, Average Loss 0.01816172994673252\n",
      "Epoch 70: Accuracy 0.820, Average Loss 1.2882721543312072\n",
      "\n",
      "Epoch 71: Accuracy 0.993, Average Loss 0.02116062004119158\n",
      "Epoch 71: Accuracy 0.815, Average Loss 1.319828987121582\n",
      "\n",
      "Epoch 72: Accuracy 0.994, Average Loss 0.016014679931104184\n",
      "Epoch 72: Accuracy 0.819, Average Loss 1.3083307862281799\n",
      "\n",
      "Epoch 73: Accuracy 0.993, Average Loss 0.020528670512139798\n",
      "Epoch 73: Accuracy 0.814, Average Loss 1.2921765446662903\n",
      "\n",
      "Epoch 74: Accuracy 0.993, Average Loss 0.019859239626675844\n",
      "Epoch 74: Accuracy 0.813, Average Loss 1.3043707251548766\n",
      "\n",
      "Epoch 75: Accuracy 0.993, Average Loss 0.017904931120574473\n",
      "Epoch 75: Accuracy 0.821, Average Loss 1.2964054822921753\n",
      "\n",
      "Epoch 76: Accuracy 0.993, Average Loss 0.01950338523834944\n",
      "Epoch 76: Accuracy 0.816, Average Loss 1.3107899188995362\n",
      "\n",
      "Epoch 77: Accuracy 0.992, Average Loss 0.02143705865368247\n",
      "Epoch 77: Accuracy 0.816, Average Loss 1.31679710149765\n",
      "\n",
      "Epoch 78: Accuracy 0.992, Average Loss 0.023796447068452835\n",
      "Epoch 78: Accuracy 0.815, Average Loss 1.3031941175460815\n",
      "\n",
      "Epoch 79: Accuracy 0.993, Average Loss 0.02078827304765582\n",
      "Epoch 79: Accuracy 0.820, Average Loss 1.2768110156059265\n",
      "\n",
      "Epoch 80: Accuracy 0.993, Average Loss 0.01892743557691574\n",
      "Epoch 80: Accuracy 0.818, Average Loss 1.2966683149337768\n",
      "\n",
      "Epoch 81: Accuracy 0.993, Average Loss 0.019882748015224935\n",
      "Epoch 81: Accuracy 0.819, Average Loss 1.2855845093727112\n",
      "\n",
      "Epoch 82: Accuracy 0.993, Average Loss 0.019216295778751374\n",
      "Epoch 82: Accuracy 0.820, Average Loss 1.2978829741477966\n",
      "\n",
      "Epoch 83: Accuracy 0.994, Average Loss 0.016790711591020226\n",
      "Epoch 83: Accuracy 0.819, Average Loss 1.3160900473594666\n",
      "\n",
      "Epoch 84: Accuracy 0.994, Average Loss 0.01782904810272157\n",
      "Epoch 84: Accuracy 0.817, Average Loss 1.3000048756599427\n",
      "\n",
      "Epoch 85: Accuracy 0.994, Average Loss 0.017396572437137367\n",
      "Epoch 85: Accuracy 0.820, Average Loss 1.3224878072738648\n",
      "\n",
      "Epoch 86: Accuracy 0.995, Average Loss 0.015962782660499216\n",
      "Epoch 86: Accuracy 0.819, Average Loss 1.30157812833786\n",
      "\n",
      "Epoch 87: Accuracy 0.994, Average Loss 0.01646688912063837\n",
      "Epoch 87: Accuracy 0.817, Average Loss 1.2916645288467408\n",
      "\n",
      "Epoch 88: Accuracy 0.993, Average Loss 0.0189503232575953\n",
      "Epoch 88: Accuracy 0.815, Average Loss 1.3057769536972046\n",
      "\n",
      "Epoch 89: Accuracy 0.994, Average Loss 0.017658201735466717\n",
      "Epoch 89: Accuracy 0.821, Average Loss 1.2963558435440063\n",
      "\n",
      "Epoch 90: Accuracy 0.993, Average Loss 0.018218978438526393\n",
      "Epoch 90: Accuracy 0.819, Average Loss 1.3059327006340027\n",
      "\n",
      "Epoch 91: Accuracy 0.994, Average Loss 0.017201338950544594\n",
      "Epoch 91: Accuracy 0.816, Average Loss 1.3179473400115966\n",
      "\n",
      "Epoch 92: Accuracy 0.994, Average Loss 0.01638261679559946\n",
      "Epoch 92: Accuracy 0.820, Average Loss 1.3022881984710692\n",
      "\n",
      "Epoch 93: Accuracy 0.993, Average Loss 0.01831278929486871\n",
      "Epoch 93: Accuracy 0.819, Average Loss 1.3165640234947205\n",
      "\n",
      "Epoch 94: Accuracy 0.993, Average Loss 0.021101053301244975\n",
      "Epoch 94: Accuracy 0.815, Average Loss 1.3307867527008057\n",
      "\n",
      "Epoch 95: Accuracy 0.994, Average Loss 0.01735193483531475\n",
      "Epoch 95: Accuracy 0.818, Average Loss 1.292070710659027\n",
      "\n",
      "Epoch 96: Accuracy 0.994, Average Loss 0.016935464441776276\n",
      "Epoch 96: Accuracy 0.816, Average Loss 1.3406854033470155\n",
      "\n",
      "Epoch 97: Accuracy 0.994, Average Loss 0.015619971975684165\n",
      "Epoch 97: Accuracy 0.817, Average Loss 1.3316312193870545\n",
      "\n",
      "Epoch 98: Accuracy 0.994, Average Loss 0.017012732354924082\n",
      "Epoch 98: Accuracy 0.816, Average Loss 1.3185505867004395\n",
      "\n",
      "Epoch 99: Accuracy 0.993, Average Loss 0.019569405708462\n",
      "Epoch 99: Accuracy 0.817, Average Loss 1.3250302791595459\n",
      "\n",
      "Epoch 100: Accuracy 0.994, Average Loss 0.01693254880607128\n",
      "Epoch 100: Accuracy 0.820, Average Loss 1.3236023545265199\n",
      "\n",
      "Epoch 101: Accuracy 0.993, Average Loss 0.020559944026172162\n",
      "Epoch 101: Accuracy 0.815, Average Loss 1.3040110349655152\n",
      "\n",
      "Epoch 102: Accuracy 0.993, Average Loss 0.01919111793860793\n",
      "Epoch 102: Accuracy 0.819, Average Loss 1.31748468875885\n",
      "\n",
      "Epoch 103: Accuracy 0.992, Average Loss 0.021892856191843748\n",
      "Epoch 103: Accuracy 0.816, Average Loss 1.3201165080070496\n",
      "\n",
      "Epoch 104: Accuracy 0.990, Average Loss 0.026836302122101186\n",
      "Epoch 104: Accuracy 0.817, Average Loss 1.3515958309173584\n",
      "\n",
      "Epoch 105: Accuracy 0.993, Average Loss 0.01991382141597569\n",
      "Epoch 105: Accuracy 0.816, Average Loss 1.3389908790588378\n",
      "\n",
      "Epoch 106: Accuracy 0.993, Average Loss 0.019287530593574048\n",
      "Epoch 106: Accuracy 0.817, Average Loss 1.3368718147277832\n",
      "\n",
      "Epoch 107: Accuracy 0.994, Average Loss 0.016763510406017302\n",
      "Epoch 107: Accuracy 0.817, Average Loss 1.3294248700141906\n",
      "\n",
      "Epoch 108: Accuracy 0.993, Average Loss 0.01958074949681759\n",
      "Epoch 108: Accuracy 0.817, Average Loss 1.3261560320854187\n",
      "\n",
      "Epoch 109: Accuracy 0.994, Average Loss 0.01611504768021405\n",
      "Epoch 109: Accuracy 0.815, Average Loss 1.3436521649360658\n",
      "\n",
      "Epoch 110: Accuracy 0.994, Average Loss 0.01708243696950376\n",
      "Epoch 110: Accuracy 0.814, Average Loss 1.3322719097137452\n",
      "\n",
      "Epoch 111: Accuracy 0.993, Average Loss 0.019432083256542682\n",
      "Epoch 111: Accuracy 0.816, Average Loss 1.3672952890396117\n",
      "\n",
      "Epoch 112: Accuracy 0.994, Average Loss 0.018073874851688744\n",
      "Epoch 112: Accuracy 0.817, Average Loss 1.3669393539428711\n",
      "\n",
      "Epoch 113: Accuracy 0.994, Average Loss 0.018693879675120116\n",
      "Epoch 113: Accuracy 0.819, Average Loss 1.342718243598938\n",
      "\n",
      "Epoch 114: Accuracy 0.993, Average Loss 0.018697620322927832\n",
      "Epoch 114: Accuracy 0.820, Average Loss 1.3401538372039794\n",
      "\n",
      "Epoch 115: Accuracy 0.992, Average Loss 0.02225121820345521\n",
      "Epoch 115: Accuracy 0.814, Average Loss 1.3515891671180724\n",
      "\n",
      "Epoch 116: Accuracy 0.992, Average Loss 0.021354997958987952\n",
      "Epoch 116: Accuracy 0.810, Average Loss 1.344226312637329\n",
      "\n",
      "Epoch 117: Accuracy 0.993, Average Loss 0.018986759558320045\n",
      "Epoch 117: Accuracy 0.816, Average Loss 1.345216739177704\n",
      "\n",
      "Epoch 118: Accuracy 0.993, Average Loss 0.02137009974103421\n",
      "Epoch 118: Accuracy 0.816, Average Loss 1.3245132684707641\n",
      "\n",
      "Epoch 119: Accuracy 0.993, Average Loss 0.018884669356048107\n",
      "Epoch 119: Accuracy 0.814, Average Loss 1.3296513915061952\n",
      "\n",
      "Epoch 120: Accuracy 0.994, Average Loss 0.01642603819258511\n",
      "Epoch 120: Accuracy 0.819, Average Loss 1.3217400550842284\n",
      "\n",
      "Epoch 121: Accuracy 0.994, Average Loss 0.018161627668887376\n",
      "Epoch 121: Accuracy 0.816, Average Loss 1.331787884235382\n",
      "\n",
      "Epoch 122: Accuracy 0.994, Average Loss 0.01674211925826967\n",
      "Epoch 122: Accuracy 0.818, Average Loss 1.315933609008789\n",
      "\n",
      "Epoch 123: Accuracy 0.994, Average Loss 0.016711291298270224\n",
      "Epoch 123: Accuracy 0.818, Average Loss 1.3696483731269837\n",
      "\n",
      "Epoch 124: Accuracy 0.993, Average Loss 0.01883565675467253\n",
      "Epoch 124: Accuracy 0.818, Average Loss 1.3555172681808472\n",
      "\n",
      "Epoch 125: Accuracy 0.994, Average Loss 0.018270140923559667\n",
      "Epoch 125: Accuracy 0.819, Average Loss 1.3244077682495117\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 190\u001B[39m\n\u001B[32m    187\u001B[39m optimizer.bind_loss_fn(cross_entropy_loss)\n\u001B[32m    188\u001B[39m optimizer.bind_network(network)\n\u001B[32m--> \u001B[39m\u001B[32m190\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 145\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(train_data, epochs, batch_size, test_data, cb)\u001B[39m\n\u001B[32m    142\u001B[39m history[\u001B[33m\"\u001B[39m\u001B[33mtest_accuracy\u001B[39m\u001B[33m\"\u001B[39m].append(test_accuracy)\n\u001B[32m    144\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m1\u001B[39m, epochs + \u001B[32m1\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m145\u001B[39m     \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    147\u001B[39m     _, train_accuracy, train_loss = eval_model(network, batched_train_data, epoch=epoch)\n\u001B[32m    148\u001B[39m     _, test_accuracy, test_loss = eval_model(network, batched_test_data, epoch=epoch)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 128\u001B[39m, in \u001B[36mtrain.<locals>.train_epoch\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    126\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtrain_epoch\u001B[39m():\n\u001B[32m    127\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m batched_train_data:\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m         x_batch = \u001B[43mmx\u001B[49m\u001B[43m.\u001B[49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mimage\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    129\u001B[39m         y_batch = mx.array(batch[\u001B[33m\"\u001B[39m\u001B[33mlabel\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m    130\u001B[39m         optimizer.step(x_batch, y_batch)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:43:41.999873Z",
     "start_time": "2025-12-14T09:43:41.948166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save and write checkpoint\n",
    "checkpoint = network.save()\n",
    "print(checkpoint.params)\n",
    "checkpoint.write(\"checkpoints/simple_conv.pb\")"
   ],
   "id": "dc1358e35c7b2a69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weight': <softgrad.layer.Layer.Layer.Parameter object at 0x11f2152b0>, 'bias': <softgrad.layer.Layer.Layer.Parameter object at 0x11f2155b0>}, {}, {}, {'weight': <softgrad.layer.Layer.Layer.Parameter object at 0x11f215a30>, 'bias': <softgrad.layer.Layer.Layer.Parameter object at 0x11f215eb0>}, {}, {}, {}, {'W': <softgrad.layer.Layer.Layer.Parameter object at 0x11f216330>, 'b': <softgrad.layer.Layer.Layer.Parameter object at 0x11f2167b0>}, {}, {'W': <softgrad.layer.Layer.Layer.Parameter object at 0x11f216c30>, 'b': <softgrad.layer.Layer.Layer.Parameter object at 0x11f2170b0>}]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T08:56:30.068725Z",
     "start_time": "2025-12-14T08:56:30.050564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load checkpoint\n",
    "network_copy = Network(input_shape=(32, 32, 3))\n",
    "\n",
    "# conv block 1\n",
    "network_copy.add_layer(MLX(nn.Conv2d(in_channels=3, out_channels=96, kernel_size=7)))\n",
    "network_copy.add_layer(Activation(leaky_relu))\n",
    "network_copy.add_layer(MLX(nn.MaxPool2d(2)))\n",
    "# conv block 2\n",
    "network_copy.add_layer(MLX(nn.Conv2d(in_channels=96, out_channels=256, kernel_size=3)))\n",
    "network_copy.add_layer(Activation(leaky_relu))\n",
    "network_copy.add_layer(MLX(nn.MaxPool2d(2)))\n",
    "# feed forward\n",
    "network_copy.add_layer(Flatten())\n",
    "network_copy.add_layer(Linear(1024))\n",
    "network_copy.add_layer(Activation(leaky_relu))\n",
    "network_copy.add_layer(Linear(10))\n",
    "\n",
    "network_copy.load(checkpoint)"
   ],
   "id": "7822202eef81888c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T08:57:07.121215Z",
     "start_time": "2025-12-14T08:56:32.649676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# eval original and checkpoint to check they're equivalent\n",
    "test_data.reset()\n",
    "eval_model(network, test_data.batch(1))\n",
    "print()\n",
    "\n",
    "test_data.reset()\n",
    "eval_model(network_copy, test_data.batch(1))\n",
    "print()"
   ],
   "id": "352965e3ed16b5e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.808, Average Loss 0.9506390641133244\n",
      "Accuracy 0.808, Average Loss 0.9506390641133244\n",
      "\n",
      "Accuracy 0.808, Average Loss 0.9506390641133244\n",
      "Accuracy 0.808, Average Loss 0.9506390641133244\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
