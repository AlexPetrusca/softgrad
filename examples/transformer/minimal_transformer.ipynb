{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-19T08:24:42.412130Z",
     "start_time": "2025-12-19T08:05:48.011437Z"
    }
   },
   "source": [
    "import math\n",
    "\n",
    "import mlx.core as mx\n",
    "import numpy as np\n",
    "from mlx import nn\n",
    "\n",
    "from softgrad import Network\n",
    "from softgrad.function.activation import Relu, Softmax, softmax\n",
    "from softgrad.function.core import Add, Concatenate\n",
    "from softgrad.function.loss import CrossEntropyLoss, sequence_ce_loss\n",
    "from softgrad.layer.attn import CausalSelfAttention\n",
    "from softgrad.layer.core import Parallel, Embedding, Sequential, Linear, Residual, Activation\n",
    "from softgrad.layer.norm import LayerNorm\n",
    "from softgrad.layer.shim import MLX\n",
    "from softgrad.layer.transform.PositionIndices import PositionIndices\n",
    "from softgrad.optim import SGD\n",
    "\n",
    "\n",
    "class MLXCausalSelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "\n",
    "        self.n_heads = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.causal_mask = MLXCausalSelfAttention.create_additive_causal_mask(block_size, dtype=mx.bfloat16)\n",
    "\n",
    "        self.query_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.key_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.value_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.out_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # calculate query, key, value for all heads\n",
    "        q = self.query_proj(x) # (B, T, C) -> (B, T, C)\n",
    "        k = self.key_proj(x) # (B, T, C) -> (B, T, C)\n",
    "        v = self.value_proj(x) # (B, T, C) -> (B, T, C)\n",
    "\n",
    "        # reshape query, key, value to batch over n_batches x n_heads\n",
    "        #   - this way we can compute attention for all heads at once (i.e. multi-head attention) with a single matrix multiply\n",
    "        #   - nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        q = mx.unflatten(q, -1, (self.n_heads, -1)).transpose(0, 2, 1, 3) # (B, T, C) -> (B, T, nh, hs) -> (B, nh, T, hs)\n",
    "        k = mx.unflatten(k, -1, (self.n_heads, -1)).transpose(0, 2, 1, 3) # (B, T, C) -> (B, T, nh, hs) -> (B, nh, T, hs)\n",
    "        v = mx.unflatten(v, -1, (self.n_heads, -1)).transpose(0, 2, 1, 3) # (B, T, C) -> (B, T, nh, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        # causal flash attention\n",
    "        scale = math.sqrt(1 / q.shape[-1])\n",
    "        output = mx.fast.scaled_dot_product_attention(q, k, v, scale=scale, mask=self.causal_mask[:T, :T]) # 3x(B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        # re-assemble all head outputs side by side and project out\n",
    "        output = output.transpose(0, 2, 1, 3).flatten(-2, -1) # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, C)\n",
    "        return self.out_proj(output) # (B, T, C) -> (B, T, C)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_additive_causal_mask(N: int, dtype = mx.float32):\n",
    "        indices = mx.arange(N)\n",
    "        mask = indices[:, None] < indices[None]\n",
    "        mask = mask.astype(dtype) * mx.finfo(dtype).min\n",
    "        return mask\n",
    "\n",
    "\n",
    "class FeedForward(Sequential):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__([\n",
    "            Linear(4 * n_embd),\n",
    "            Activation(Relu()),\n",
    "            Linear(n_embd)\n",
    "        ])\n",
    "\n",
    "\n",
    "class MultiHeadAttention(Sequential):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__([\n",
    "            Parallel(\n",
    "                [CausalSelfAttention(n_embd, head_size, block_size) for _ in range(num_heads)]  # heads\n",
    "            , Concatenate()),\n",
    "            Linear(n_embd)  # projection\n",
    "        ])\n",
    "\n",
    "\n",
    "class TransformerBlock(Sequential):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__([\n",
    "            # communication\n",
    "            Residual(Sequential([\n",
    "                LayerNorm(),\n",
    "                MultiHeadAttention(n_head, n_embd // n_head)\n",
    "                # MLX(MLXCausalSelfAttention())\n",
    "            ])),\n",
    "            # computation\n",
    "            Residual(Sequential([\n",
    "                LayerNorm(),\n",
    "                FeedForward(n_embd)\n",
    "            ]))\n",
    "        ])\n",
    "\n",
    "\n",
    "mx.random.seed(1337)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Hyperparameters\n",
    "# ----------------------------------------------------------------------------------\n",
    "batch_size = 32\n",
    "block_size = 256\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-2\n",
    "eval_iters = 50\n",
    "n_embd = 256\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Load Dataset\n",
    "# ----------------------------------------------------------------------------------\n",
    "with open('rsc/tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = mx.array(encode(text))\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = mx.random.randint(0, len(data_split) - block_size, (batch_size,))\n",
    "    x = mx.stack([data_split[int(i):int(i) + block_size] for i in ix])\n",
    "    y = mx.stack([data_split[int(i) + 1:int(i) + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def generate_text(network, start_text=\"\", max_new_tokens=500, temperature=1.0, top_k=None):\n",
    "    if start_text:\n",
    "        context = encode(start_text)\n",
    "    else:\n",
    "        context = [0]\n",
    "\n",
    "    context = list(context)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        if len(context) < block_size:\n",
    "            context_padded = [0] * (block_size - len(context)) + context  # pad 0s on the left\n",
    "        else:\n",
    "            context_padded = context[-block_size:]  # take as much as we can fit into context\n",
    "\n",
    "        context_array = mx.array(context_padded)[None, :]  # (1, block_size)\n",
    "        logits = network.forward(context_array, save_ctx=False)  # (1, block_size, vocab_size)\n",
    "\n",
    "        if len(context) < block_size:\n",
    "            logits = logits[:, len(context) - 1, :]  # (1, vocab_size)\n",
    "        else:\n",
    "            logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "\n",
    "        logits = logits / temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_values = mx.sort(logits[0])[-top_k:]\n",
    "            threshold = top_values[0]\n",
    "            logits_filtered = mx.where(logits[0] >= threshold, logits[0], float('-inf'))\n",
    "            logits = logits_filtered[None, :]\n",
    "\n",
    "        probs = mx.softmax(logits, axis=-1) # convert to probabilities\n",
    "        idx_next = mx.random.categorical(mx.log(probs[0]), num_samples=1) # sample from distribution\n",
    "        context.append(int(idx_next[0]))\n",
    "\n",
    "    if start_text:\n",
    "        generated_tokens = context[len(encode(start_text)):]\n",
    "    else:\n",
    "        generated_tokens = context[1:]\n",
    "\n",
    "    return decode(generated_tokens)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Setup Network\n",
    "# ----------------------------------------------------------------------------------\n",
    "network = Network(input_shape=(block_size,))\n",
    "network.add_layer(Parallel([\n",
    "    Embedding(vocab_size, n_embd),  # Semantic encoding\n",
    "    Sequential([\n",
    "        PositionIndices(),\n",
    "        Embedding(block_size, n_embd)  # Positional encoding\n",
    "    ])\n",
    "], Add()))\n",
    "network.add_layer(Sequential(\n",
    "    [TransformerBlock(n_embd, n_head) for _ in range(n_layer)]  # transformer blocks\n",
    "))\n",
    "network.add_layer(LayerNorm())\n",
    "network.add_layer(Linear(vocab_size))  # LLM head\n",
    "\n",
    "optimizer = SGD(eta=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer.bind_loss_fn(sequence_ce_loss)\n",
    "optimizer.bind_network(network)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Evaluation function\n",
    "# ----------------------------------------------------------------------------------\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = []\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "\n",
    "            # forward pass\n",
    "            logits = network.forward(X, save_ctx=False)\n",
    "\n",
    "            # compute loss\n",
    "            loss_per_token = sequence_ce_loss.apply(logits, Y)\n",
    "            mean_loss = mx.mean(loss_per_token)\n",
    "\n",
    "            losses.append(mean_loss.item())\n",
    "\n",
    "        out[split] = np.mean(losses)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Train Loop\n",
    "# ----------------------------------------------------------------------------------\n",
    "print(\"Training...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter:4d}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    optimizer.step(xb, yb)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Final Evaluation\n",
    "# ----------------------------------------------------------------------------------\n",
    "losses = estimate_loss()\n",
    "print(f\"Final: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "prompts = [\n",
    "    \"ROMEO:\",\n",
    "    \"To be or not to be\",\n",
    "    \"First Citizen:\\n\",\n",
    "    \"The king\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"-\" * 40)\n",
    "    generated = generate_text(\n",
    "        network,\n",
    "        start_text=prompt,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.8,\n",
    "        top_k=40\n",
    "    )\n",
    "    print(prompt + generated)\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "--------------------------------------------------\n",
      "step    0: train loss 4.3638, val loss 4.3552\n",
      "step  100: train loss 3.2845, val loss 3.3251\n",
      "step  200: train loss 3.1693, val loss 3.2127\n",
      "step  300: train loss 2.9192, val loss 2.9483\n",
      "step  400: train loss 2.7731, val loss 2.7905\n",
      "step  500: train loss 2.6946, val loss 2.7135\n",
      "step  600: train loss 2.6488, val loss 2.6597\n",
      "step  700: train loss 2.6190, val loss 2.6262\n",
      "step  800: train loss 2.5955, val loss 2.6084\n",
      "step  900: train loss 2.5808, val loss 2.5860\n",
      "step 1000: train loss 2.5655, val loss 2.5707\n",
      "step 1100: train loss 2.5494, val loss 2.5558\n",
      "step 1200: train loss 2.5418, val loss 2.5430\n",
      "step 1300: train loss 2.5304, val loss 2.5376\n",
      "step 1400: train loss 2.5214, val loss 2.5270\n",
      "step 1500: train loss 2.5118, val loss 2.5235\n",
      "step 1600: train loss 2.5060, val loss 2.5156\n",
      "step 1700: train loss 2.4980, val loss 2.5044\n",
      "step 1800: train loss 2.4966, val loss 2.5014\n",
      "step 1900: train loss 2.4848, val loss 2.4934\n",
      "step 2000: train loss 2.4827, val loss 2.4896\n",
      "step 2100: train loss 2.4764, val loss 2.4818\n",
      "step 2200: train loss 2.4720, val loss 2.4810\n",
      "step 2300: train loss 2.4688, val loss 2.4778\n",
      "step 2400: train loss 2.4673, val loss 2.4747\n",
      "step 2500: train loss 2.4595, val loss 2.4679\n",
      "step 2600: train loss 2.4565, val loss 2.4703\n",
      "step 2700: train loss 2.4538, val loss 2.4581\n",
      "step 2800: train loss 2.4462, val loss 2.4557\n",
      "step 2900: train loss 2.4424, val loss 2.4587\n",
      "step 3000: train loss 2.4425, val loss 2.4536\n",
      "step 3100: train loss 2.4392, val loss 2.4467\n",
      "step 3200: train loss 2.4357, val loss 2.4511\n",
      "step 3300: train loss 2.4327, val loss 2.4466\n",
      "step 3400: train loss 2.4253, val loss 2.4429\n",
      "step 3500: train loss 2.4272, val loss 2.4433\n",
      "step 3600: train loss 2.4177, val loss 2.4322\n",
      "step 3700: train loss 2.4170, val loss 2.4339\n",
      "step 3800: train loss 2.4104, val loss 2.4262\n",
      "step 3900: train loss 2.4084, val loss 2.4276\n",
      "step 4000: train loss 2.4086, val loss 2.4222\n",
      "step 4100: train loss 2.4028, val loss 2.4198\n",
      "step 4200: train loss 2.3997, val loss 2.4177\n",
      "step 4300: train loss 2.4003, val loss 2.4191\n",
      "step 4400: train loss 2.3968, val loss 2.4123\n",
      "step 4500: train loss 2.3921, val loss 2.4095\n",
      "step 4600: train loss 2.3900, val loss 2.4105\n",
      "step 4700: train loss 2.3890, val loss 2.4100\n",
      "step 4800: train loss 2.3860, val loss 2.4037\n",
      "step 4900: train loss 2.3777, val loss 2.3932\n",
      "Final: train loss 2.3773, val loss 2.3995\n",
      "\n",
      "Prompt: 'ROMEO:'\n",
      "----------------------------------------\n",
      "ROMEO: rT\n",
      "HOCSOTTSO!LICI!ANUDGLOSNSQKASNLLSSXSNFHRUENILSJSSSGSTNqC'NGLGUUNWNNGS$CESSCTMD!SDBTLK3CNWN3ITLSUSLDCLSHIKTLIMIGNSLSPSVQLwSRSrIONUCQS.TL' ItLQCeVG \n",
      "\n",
      "\n",
      "Prompt: 'To be or not to be'\n",
      "----------------------------------------\n",
      "To be or not to beSSESUtSASC3S!OGNU'SEYINCGJNWNISSDSUT tLYNSLITBSOIILCUSrLqAFIUSGSCUNIS'SLS3DIxNTISCCCKCUSSCSECCGITSTCLNSTNN\n",
      "UCCNUSSANCULCN:ENLLUNENRiQC QQS:C':CLIES SG\n",
      "\n",
      "\n",
      "Prompt: 'First Citizen:\n",
      "'\n",
      "----------------------------------------\n",
      "First Citizen:\n",
      "NCINIITLSWWNINOLNSTGIPSCSILOOS3FCSLGU GCMLCSSSOEGSQANXKNSUDTSSWNTCGCCISASqLTOGSCASCSTLESULHINFQHCCLRIKCASSSLNSDCECtISNUU\n",
      "L?T'UUE\n",
      "$EISIUZ ILAS I:.\n",
      "\n",
      "CQI\n",
      "\n",
      "\n",
      "Prompt: 'The king'\n",
      "----------------------------------------\n",
      "The kingCCOBNtSt3AKSNLOSTXLKWJNNHNLDTLIGANCN:ENUNNRFGFZTOCOSNSSQCItSGSSCSSSOCSDQSSNTBSUUVALASONLSLISQTHFECCH3SSSGLKSLHNI::NKRNSCSrWCNGSR.NSDEOLMASNQ$RO?LT K\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T10:28:58.784405Z",
     "start_time": "2025-12-19T08:54:32.082006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train\n",
    "for iter in range(25000):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter:4d}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    optimizer.step(xb, yb)"
   ],
   "id": "87b36039a4c746b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0: train loss 2.2142, val loss 2.2496\n",
      "step  100: train loss 2.2134, val loss 2.2428\n",
      "step  200: train loss 2.2078, val loss 2.2370\n",
      "step  300: train loss 2.2017, val loss 2.2297\n",
      "step  400: train loss 2.1929, val loss 2.2307\n",
      "step  500: train loss 2.2009, val loss 2.2377\n",
      "step  600: train loss 2.1875, val loss 2.2172\n",
      "step  700: train loss 2.1794, val loss 2.2122\n",
      "step  800: train loss 2.1706, val loss 2.2070\n",
      "step  900: train loss 2.1714, val loss 2.2022\n",
      "step 1000: train loss 2.1539, val loss 2.1833\n",
      "step 1100: train loss 2.1517, val loss 2.1930\n",
      "step 1200: train loss 2.1423, val loss 2.1808\n",
      "step 1300: train loss 2.1432, val loss 2.1758\n",
      "step 1400: train loss 2.1359, val loss 2.1737\n",
      "step 1500: train loss 2.1273, val loss 2.1684\n",
      "step 1600: train loss 2.1236, val loss 2.1568\n",
      "step 1700: train loss 2.1258, val loss 2.1565\n",
      "step 1800: train loss 2.1068, val loss 2.1495\n",
      "step 1900: train loss 2.1054, val loss 2.1460\n",
      "step 2000: train loss 2.1049, val loss 2.1471\n",
      "step 2100: train loss 2.0944, val loss 2.1302\n",
      "step 2200: train loss 2.0870, val loss 2.1322\n",
      "step 2300: train loss 2.0826, val loss 2.1313\n",
      "step 2400: train loss 2.0711, val loss 2.1185\n",
      "step 2500: train loss 2.0700, val loss 2.1116\n",
      "step 2600: train loss 2.0576, val loss 2.0981\n",
      "step 2700: train loss 2.0515, val loss 2.0917\n",
      "step 2800: train loss 2.0452, val loss 2.0821\n",
      "step 2900: train loss 2.0379, val loss 2.0846\n",
      "step 3000: train loss 2.0326, val loss 2.0793\n",
      "step 3100: train loss 2.0186, val loss 2.0654\n",
      "step 3200: train loss 2.0120, val loss 2.0604\n",
      "step 3300: train loss 2.0070, val loss 2.0611\n",
      "step 3400: train loss 2.0023, val loss 2.0518\n",
      "step 3500: train loss 1.9998, val loss 2.0534\n",
      "step 3600: train loss 1.9849, val loss 2.0461\n",
      "step 3700: train loss 1.9906, val loss 2.0455\n",
      "step 3800: train loss 1.9792, val loss 2.0363\n",
      "step 3900: train loss 1.9634, val loss 2.0195\n",
      "step 4000: train loss 1.9641, val loss 2.0198\n",
      "step 4100: train loss 1.9529, val loss 2.0116\n",
      "step 4200: train loss 1.9641, val loss 2.0209\n",
      "step 4300: train loss 1.9478, val loss 2.0091\n",
      "step 4400: train loss 1.9381, val loss 1.9958\n",
      "step 4500: train loss 1.9256, val loss 1.9941\n",
      "step 4600: train loss 1.9294, val loss 1.9953\n",
      "step 4700: train loss 1.9199, val loss 1.9815\n",
      "step 4800: train loss 1.9255, val loss 1.9860\n",
      "step 4900: train loss 1.9122, val loss 1.9763\n",
      "step 5000: train loss 1.9040, val loss 1.9724\n",
      "step 5100: train loss 1.8989, val loss 1.9683\n",
      "step 5200: train loss 1.8868, val loss 1.9609\n",
      "step 5300: train loss 1.8854, val loss 1.9563\n",
      "step 5400: train loss 1.8825, val loss 1.9580\n",
      "step 5500: train loss 1.8774, val loss 1.9553\n",
      "step 5600: train loss 1.8749, val loss 1.9472\n",
      "step 5700: train loss 1.8590, val loss 1.9350\n",
      "step 5800: train loss 1.8626, val loss 1.9377\n",
      "step 5900: train loss 1.8602, val loss 1.9323\n",
      "step 6000: train loss 1.8481, val loss 1.9299\n",
      "step 6100: train loss 1.8407, val loss 1.9210\n",
      "step 6200: train loss 1.8351, val loss 1.9192\n",
      "step 6300: train loss 1.8287, val loss 1.9142\n",
      "step 6400: train loss 1.8300, val loss 1.9142\n",
      "step 6500: train loss 1.8323, val loss 1.9169\n",
      "step 6600: train loss 1.8124, val loss 1.8954\n",
      "step 6700: train loss 1.8134, val loss 1.9029\n",
      "step 6800: train loss 1.8060, val loss 1.9004\n",
      "step 6900: train loss 1.8033, val loss 1.8981\n",
      "step 7000: train loss 1.7950, val loss 1.8865\n",
      "step 7100: train loss 1.7901, val loss 1.8859\n",
      "step 7200: train loss 1.7813, val loss 1.8868\n",
      "step 7300: train loss 1.7861, val loss 1.8928\n",
      "step 7400: train loss 1.7732, val loss 1.8779\n",
      "step 7500: train loss 1.7678, val loss 1.8692\n",
      "step 7600: train loss 1.7672, val loss 1.8744\n",
      "step 7700: train loss 1.7600, val loss 1.8565\n",
      "step 7800: train loss 1.7555, val loss 1.8679\n",
      "step 7900: train loss 1.7508, val loss 1.8580\n",
      "step 8000: train loss 1.7492, val loss 1.8567\n",
      "step 8100: train loss 1.7432, val loss 1.8535\n",
      "step 8200: train loss 1.7374, val loss 1.8582\n",
      "step 8300: train loss 1.7397, val loss 1.8587\n",
      "step 8400: train loss 1.7382, val loss 1.8633\n",
      "step 8500: train loss 1.7200, val loss 1.8440\n",
      "step 8600: train loss 1.7296, val loss 1.8409\n",
      "step 8700: train loss 1.7138, val loss 1.8326\n",
      "step 8800: train loss 1.7102, val loss 1.8370\n",
      "step 8900: train loss 1.7207, val loss 1.8438\n",
      "step 9000: train loss 1.7031, val loss 1.8215\n",
      "step 9100: train loss 1.7034, val loss 1.8352\n",
      "step 9200: train loss 1.7090, val loss 1.8397\n",
      "step 9300: train loss 1.6981, val loss 1.8268\n",
      "step 9400: train loss 1.6969, val loss 1.8180\n",
      "step 9500: train loss 1.6829, val loss 1.8143\n",
      "step 9600: train loss 1.6749, val loss 1.8103\n",
      "step 9700: train loss 1.6672, val loss 1.8097\n",
      "step 9800: train loss 1.6724, val loss 1.8086\n",
      "step 9900: train loss 1.6738, val loss 1.8077\n",
      "step 10000: train loss 1.6648, val loss 1.7984\n",
      "step 10100: train loss 1.6576, val loss 1.7955\n",
      "step 10200: train loss 1.6559, val loss 1.8054\n",
      "step 10300: train loss 1.6531, val loss 1.7934\n",
      "step 10400: train loss 1.6514, val loss 1.7932\n",
      "step 10500: train loss 1.6465, val loss 1.7953\n",
      "step 10600: train loss 1.6345, val loss 1.7918\n",
      "step 10700: train loss 1.6444, val loss 1.7967\n",
      "step 10800: train loss 1.6500, val loss 1.7958\n",
      "step 10900: train loss 1.6269, val loss 1.7896\n",
      "step 11000: train loss 1.6243, val loss 1.7786\n",
      "step 11100: train loss 1.6341, val loss 1.7870\n",
      "step 11200: train loss 1.6192, val loss 1.7728\n",
      "step 11300: train loss 1.6185, val loss 1.7712\n",
      "step 11400: train loss 1.6182, val loss 1.7641\n",
      "step 11500: train loss 1.6178, val loss 1.7698\n",
      "step 11600: train loss 1.6057, val loss 1.7663\n",
      "step 11700: train loss 1.6020, val loss 1.7683\n",
      "step 11800: train loss 1.5997, val loss 1.7660\n",
      "step 11900: train loss 1.6057, val loss 1.7601\n",
      "step 12000: train loss 1.5969, val loss 1.7672\n",
      "step 12100: train loss 1.5928, val loss 1.7595\n",
      "step 12200: train loss 1.5903, val loss 1.7559\n",
      "step 12300: train loss 1.5843, val loss 1.7590\n",
      "step 12400: train loss 1.5931, val loss 1.7495\n",
      "step 12500: train loss 1.5804, val loss 1.7438\n",
      "step 12600: train loss 1.5804, val loss 1.7523\n",
      "step 12700: train loss 1.5769, val loss 1.7474\n",
      "step 12800: train loss 1.5786, val loss 1.7424\n",
      "step 12900: train loss 1.5692, val loss 1.7284\n",
      "step 13000: train loss 1.5673, val loss 1.7367\n",
      "step 13100: train loss 1.5626, val loss 1.7322\n",
      "step 13200: train loss 1.5573, val loss 1.7285\n",
      "step 13300: train loss 1.5611, val loss 1.7318\n",
      "step 13400: train loss 1.5633, val loss 1.7343\n",
      "step 13500: train loss 1.5487, val loss 1.7281\n",
      "step 13600: train loss 1.5580, val loss 1.7375\n",
      "step 13700: train loss 1.5592, val loss 1.7338\n",
      "step 13800: train loss 1.5470, val loss 1.7239\n",
      "step 13900: train loss 1.5488, val loss 1.7281\n",
      "step 14000: train loss 1.5327, val loss 1.7097\n",
      "step 14100: train loss 1.5320, val loss 1.7132\n",
      "step 14200: train loss 1.5330, val loss 1.7051\n",
      "step 14300: train loss 1.5446, val loss 1.7255\n",
      "step 14400: train loss 1.5317, val loss 1.7050\n",
      "step 14500: train loss 1.5268, val loss 1.7121\n",
      "step 14600: train loss 1.5301, val loss 1.7150\n",
      "step 14700: train loss 1.5213, val loss 1.7020\n",
      "step 14800: train loss 1.5220, val loss 1.7100\n",
      "step 14900: train loss 1.5167, val loss 1.6910\n",
      "step 15000: train loss 1.5054, val loss 1.6940\n",
      "step 15100: train loss 1.5141, val loss 1.6997\n",
      "step 15200: train loss 1.5152, val loss 1.6981\n",
      "step 15300: train loss 1.5020, val loss 1.6910\n",
      "step 15400: train loss 1.5122, val loss 1.6872\n",
      "step 15500: train loss 1.5055, val loss 1.6901\n",
      "step 15600: train loss 1.4991, val loss 1.6813\n",
      "step 15700: train loss 1.5040, val loss 1.7048\n",
      "step 15800: train loss 1.4981, val loss 1.6885\n",
      "step 15900: train loss 1.4940, val loss 1.6840\n",
      "step 16000: train loss 1.4862, val loss 1.6774\n",
      "step 16100: train loss 1.4904, val loss 1.6729\n",
      "step 16200: train loss 1.5046, val loss 1.6797\n",
      "step 16300: train loss 1.4870, val loss 1.6817\n",
      "step 16400: train loss 1.4773, val loss 1.6681\n",
      "step 16500: train loss 1.4739, val loss 1.6711\n",
      "step 16600: train loss 1.4841, val loss 1.6662\n",
      "step 16700: train loss 1.4732, val loss 1.6642\n",
      "step 16800: train loss 1.4670, val loss 1.6645\n",
      "step 16900: train loss 1.4755, val loss 1.6666\n",
      "step 17000: train loss 1.4682, val loss 1.6577\n",
      "step 17100: train loss 1.4658, val loss 1.6580\n",
      "step 17200: train loss 1.4663, val loss 1.6629\n",
      "step 17300: train loss 1.4583, val loss 1.6643\n",
      "step 17400: train loss 1.4625, val loss 1.6659\n",
      "step 17500: train loss 1.4809, val loss 1.6778\n",
      "step 17600: train loss 1.4529, val loss 1.6477\n",
      "step 17700: train loss 1.4518, val loss 1.6525\n",
      "step 17800: train loss 1.4473, val loss 1.6505\n",
      "step 17900: train loss 1.4437, val loss 1.6432\n",
      "step 18000: train loss 1.4521, val loss 1.6457\n",
      "step 18100: train loss 1.4402, val loss 1.6450\n",
      "step 18200: train loss 1.4452, val loss 1.6486\n",
      "step 18300: train loss 1.4496, val loss 1.6507\n",
      "step 18400: train loss 1.4486, val loss 1.6500\n",
      "step 18500: train loss 1.4397, val loss 1.6384\n",
      "step 18600: train loss 1.4415, val loss 1.6425\n",
      "step 18700: train loss 1.4290, val loss 1.6350\n",
      "step 18800: train loss 1.4427, val loss 1.6339\n",
      "step 18900: train loss 1.4258, val loss 1.6344\n",
      "step 19000: train loss 1.4288, val loss 1.6208\n",
      "step 19100: train loss 1.4188, val loss 1.6315\n",
      "step 19200: train loss 1.4211, val loss 1.6217\n",
      "step 19300: train loss 1.4307, val loss 1.6396\n",
      "step 19400: train loss 1.4297, val loss 1.6181\n",
      "step 19500: train loss 1.4275, val loss 1.6185\n",
      "step 19600: train loss 1.4213, val loss 1.6151\n",
      "step 19700: train loss 1.4186, val loss 1.6273\n",
      "step 19800: train loss 1.4287, val loss 1.6274\n",
      "step 19900: train loss 1.4228, val loss 1.6228\n",
      "step 20000: train loss 1.4135, val loss 1.6220\n",
      "step 20100: train loss 1.4216, val loss 1.6342\n",
      "step 20200: train loss 1.4086, val loss 1.6117\n",
      "step 20300: train loss 1.3977, val loss 1.6152\n",
      "step 20400: train loss 1.4070, val loss 1.6135\n",
      "step 20500: train loss 1.4098, val loss 1.6206\n",
      "step 20600: train loss 1.4102, val loss 1.6157\n",
      "step 20700: train loss 1.4126, val loss 1.6078\n",
      "step 20800: train loss 1.4031, val loss 1.6200\n",
      "step 20900: train loss 1.3965, val loss 1.6004\n",
      "step 21000: train loss 1.3831, val loss 1.6026\n",
      "step 21100: train loss 1.3887, val loss 1.5994\n",
      "step 21200: train loss 1.3849, val loss 1.6002\n",
      "step 21300: train loss 1.3904, val loss 1.6119\n",
      "step 21400: train loss 1.3855, val loss 1.5938\n",
      "step 21500: train loss 1.3876, val loss 1.6011\n",
      "step 21600: train loss 1.3836, val loss 1.5943\n",
      "step 21700: train loss 1.3956, val loss 1.6065\n",
      "step 21800: train loss 1.3968, val loss 1.5997\n",
      "step 21900: train loss 1.3753, val loss 1.5887\n",
      "step 22000: train loss 1.3774, val loss 1.5892\n",
      "step 22100: train loss 1.3888, val loss 1.5911\n",
      "step 22200: train loss 1.3858, val loss 1.5939\n",
      "step 22300: train loss 1.3826, val loss 1.5949\n",
      "step 22400: train loss 1.3710, val loss 1.5868\n",
      "step 22500: train loss 1.3758, val loss 1.5919\n",
      "step 22600: train loss 1.3779, val loss 1.5941\n",
      "step 22700: train loss 1.3812, val loss 1.5907\n",
      "step 22800: train loss 1.3757, val loss 1.5852\n",
      "step 22900: train loss 1.3638, val loss 1.5834\n",
      "step 23000: train loss 1.3822, val loss 1.5920\n",
      "step 23100: train loss 1.3777, val loss 1.5790\n",
      "step 23200: train loss 1.3709, val loss 1.5864\n",
      "step 23300: train loss 1.3591, val loss 1.5661\n",
      "step 23400: train loss 1.3570, val loss 1.5768\n",
      "step 23500: train loss 1.3597, val loss 1.5718\n",
      "step 23600: train loss 1.3619, val loss 1.5649\n",
      "step 23700: train loss 1.3534, val loss 1.5569\n",
      "step 23800: train loss 1.3636, val loss 1.5827\n",
      "step 23900: train loss 1.3530, val loss 1.5704\n",
      "step 24000: train loss 1.3518, val loss 1.5710\n",
      "step 24100: train loss 1.3457, val loss 1.5763\n",
      "step 24200: train loss 1.3510, val loss 1.5584\n",
      "step 24300: train loss 1.3601, val loss 1.5802\n",
      "step 24400: train loss 1.3524, val loss 1.5650\n",
      "step 24500: train loss 1.3518, val loss 1.5673\n",
      "step 24600: train loss 1.3444, val loss 1.5639\n",
      "step 24700: train loss 1.3431, val loss 1.5504\n",
      "step 24800: train loss 1.3455, val loss 1.5607\n",
      "step 24900: train loss 1.3372, val loss 1.5578\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T17:55:22.900362Z",
     "start_time": "2025-12-19T17:55:15.374989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate\n",
    "start_texts = [\n",
    "    \"First Citizen:\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"The \",\n",
    "]\n",
    "\n",
    "for start in start_texts:\n",
    "    generated = generate_text(network, start_text=start, max_new_tokens=500)\n",
    "    print(f\"Starting with: {repr(start)}\")\n",
    "    print(generated)\n",
    "    print(\"-\" * 80)"
   ],
   "id": "55f751086280c5d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with: 'First Citizen:\\n'\n",
      "tSCSSSLSSSnCOSSSSVSMSSSSXSSRSS SSSSSCTSTLCSSCSGLSF'FpSSSSSSSSSMLSSSSESSpSSSSLSLLCSSMSSLSCSSUntCSSSnSYSSSSpSSSSSNTfUtSSSn\n",
      "sReLOe-iYLteAAeUUSAOCSSSSHrUOC:O::j\n",
      "BSOOS::BO:oUOtCn:YD.OSY:\n",
      "O:Vt  mSLUzU:SEU\n",
      "Nto\n",
      "o ntie?t\n",
      "netaeeimiihhr oethctoifeommhoodmtH'sipedom;lyway\n",
      "Sultwongnatious.\n",
      "\n",
      "SOR CiMANIUS:\n",
      "CAUS imonably-of head.\n",
      "\n",
      "MOLY ABNmicious City.\n",
      "\n",
      "COMENIUS:\n",
      "Tell Consten, know thou't empointentents thought\n",
      "More thee, cramory'd without't.\n",
      "\n",
      "Seleave your are you;y Adier liftly.\n",
      "\n",
      "ASAMILLO:\n",
      "Ay 'twary you, well \n",
      "--------------------------------------------------------------------------------\n",
      "Starting with: '\\n\\n'\n",
      "tTr\n",
      "SSS.SSpSvwTSSSCSLStSSSCnSCLCSeRLSSSSSSSSSSSCLCSSCnSSSvCPSSCMSSSSSLPSCCpSISSCSSCSSOSST\n",
      "SSSSSSSSSSnuSnSCInSULSpBSSTCSSSCSSCCCMOSSBLUMeexAMYaSNr\n",
      "OCOAvBSEeCavOS\n",
      "i&NSVOCyOOS;Y::O:g::xvOwAO:.:OSOSSrnOm:HUtUkUeUfuOyM?e\n",
      "\n",
      " .rt\n",
      "\n",
      "e-eo!'s&nli\n",
      "ine\n",
      "ss\n",
      " etSntSOeiSpoe.ABest-moman:\n",
      "'te.\n",
      "wavinitobumantly.\n",
      "Citorely!Penious!\n",
      "\n",
      "SICICIORSIA:\n",
      "MastergeRoR.\n",
      "\n",
      "MENENIUS:\n",
      "jot thy pritcteouse.\n",
      "\n",
      "SICINIUSTEllo Citizens; woy.\n",
      "\n",
      "CORIOLANUS:\n",
      "Colize.\n",
      "\n",
      "LUC MERCUTIO:\n",
      "Mosttrance.\n",
      "\n",
      "Thim:\n",
      "Othe swart Cregy:\n",
      "on a morn.\n",
      "\n",
      "COMIONGBELLA:\n",
      "C\n",
      "--------------------------------------------------------------------------------\n",
      "Starting with: 'The '\n",
      "ctt'SSSsSCS'SRSPStRSCDBTSLSSESTSOSSSSSSSSUSySZSJSSSSSSISLS.SSSSpOSSSSSTTIp'SSScCNqSSSSSCSESCSSSMToSSUSSSSCnSSSLt\n",
      "SOSSSSSSUOpCSSSteLSCASCeCBcA\n",
      "EaBAerSvUAY:AeUO\n",
      "Y.uOOLUtUUSONoeZ:A\n",
      "?OS:EOOQ:iYE\n",
      "U:LSUt\n",
      "rvQOO\n",
      "\n",
      ":erbSSL$s\n",
      "Io.wUsS ZSieo\n",
      "S  v  \n",
      "n 'VuJCtS'ooysrIw'lainanI?\n",
      "\n",
      "CORIOUSmikesDYmutionus to tinnoumancy?\n",
      "\n",
      "CORIOLAM:\n",
      "I witblies awIt him por my to wentone.\n",
      "\n",
      "SICINIUSA:\n",
      "Callnemicius.\n",
      "\n",
      "SICINIUS:\n",
      "\n",
      "BRUTUSTER:\n",
      "Withouching goveard man.\n",
      "\n",
      "VOMUMNIANA:\n",
      "Talk notting\n",
      "Sir\n",
      "MERCUMIO:\n",
      "Why thanky touch-morrow, tribune \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T17:53:07.306235Z",
     "start_time": "2025-12-19T17:52:59.618049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate with priming\n",
    "def generate_with_priming(network, prompt=\"\", max_new_tokens=500, temperature=1.0):\n",
    "    prime_text = \"\"\"Act I. Scene I. Rome. A street.\n",
    "\n",
    "Enter a company of mutinous Citizens, with staves, clubs, and other weapons.\n",
    "\n",
    "First Citizen:\n",
    "Before we proceed any further, hear me speak.\n",
    "\n",
    "All:\n",
    "Speak, speak.\n",
    "\n",
    "First Citizen:\n",
    "You are all resolved rather to die than to famish?\n",
    "\n",
    "All:\n",
    "Resolved. Resolved.\n",
    "\n",
    "First Citizen:\n",
    "First, you know Caius Marcius is chief enemy to the people.\n",
    "\n",
    "All:\n",
    "We know't, we know't.\n",
    "\n",
    "First Citizen:\n",
    "Let us kill him, and we'll have corn at our own price.\n",
    "Is't a verdict?\n",
    "\n",
    "All:\n",
    "No more talking on't; let it be done: away, away!\n",
    "\n",
    "Second Citizen:\n",
    "One word, good citizens.\n",
    "\n",
    "First Citizen:\n",
    "We are accounted poor citizens, the patricians good.\n",
    "What authority surfeits on would relieve us: if they\n",
    "would yield us but the superfluity, while it were\n",
    "wholesome, we might guess they relieved us humanely; but\n",
    "they think we are too dear: the leanness that afflicts\n",
    "us, the object of our misery, is as an inventory to\n",
    "particularise their abundance; our sufferance is a\n",
    "gain to them Let us revenge this with our pikes, ere\n",
    "we become rakes: for the gods know I speak this in\n",
    "hunger for bread, not in thirst for revenge.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    full_start = prime_text + prompt\n",
    "    generated = generate_text(network, full_start, max_new_tokens, temperature)\n",
    "    return generated[len(prompt):]\n",
    "\n",
    "\n",
    "start_texts = [\n",
    "    \"First Citizen:\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"The \",\n",
    "]\n",
    "\n",
    "for start in start_texts:\n",
    "    generated = generate_with_priming(network, prompt=start)\n",
    "    print(f\"Starting with: {repr(start)}\")\n",
    "    print(generated)\n",
    "    print(\"-\" * 80)"
   ],
   "id": "2fe5c2c334922000",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with: 'First Citizen:\\n'\n",
      "otickly impention, I warring, he\n",
      "hath a beening him in this coal not.\n",
      "\n",
      "Both Gentlemen camive:\n",
      "He modenous lies of my wife will but fire.\n",
      "\n",
      "ROMEO:\n",
      "Warwick, sir, sir!\n",
      "\n",
      "BRUTUS:\n",
      "Will uttle crew your planting or bring is mercily.\n",
      "\n",
      "BRUTUS:\n",
      "Now call this with me.\n",
      "\n",
      "MERCUTIO:\n",
      "Marry, id morrilyr valuke I should sir.\n",
      "\n",
      "ROMEO:\n",
      "My your lord?\n",
      "\n",
      "GLOUCESTER:\n",
      "You king, off we more.\n",
      "\n",
      "ESTRESIO:\n",
      "I am not with too your soldiers were sway.\n",
      "\n",
      "Provost:\n",
      "'Tis not be your wife, sir, to your good wish.\n",
      "\n",
      "SICINIUS\n",
      "--------------------------------------------------------------------------------\n",
      "Starting with: '\\n\\n'\n",
      "EEN MARCISINA:\n",
      "Would you, we think world dilk-looking of all plago\n",
      "To hear.\n",
      "\n",
      "DUKE VI:\n",
      "Good dorsound to the be safe, who brand of thorns.\n",
      "\n",
      "RICHARD:\n",
      "What, to give may Richard\n",
      "Return; my like kind to to hear what think.\n",
      "\n",
      "ROMEO:\n",
      "There on talk'st\n",
      "To troublat be admost: I do not lamour retol,\n",
      "Nor to delign never righten from them knifes.\n",
      "\n",
      "ROMEO:\n",
      "I flower love, say that too wash; thoughtst reports torn.\n",
      "\n",
      "ROMEO:\n",
      "O, getter will better thou love my find;\n",
      "On mine argels show shall them ballad be shouldes\n",
      "--------------------------------------------------------------------------------\n",
      "Starting with: 'The '\n",
      "l credition of the traitor of steps are,\n",
      "Even wives with trouble men to from this:\n",
      "But if it is stufficial points to blow\n",
      "I prove the most love, I would not which,\n",
      "him, to great me to fling to shrild; that is it mattle,\n",
      "And seeming to the pride pitercipe of hate\n",
      "I can that thou shalt came of this west of a kit\n",
      "to the aften for mine about off.\n",
      "\n",
      "ROMEO:\n",
      "\n",
      "Floot off, lords:\n",
      "It have way in too?\n",
      "\n",
      "PETER:\n",
      "Art, yet more wrant. To tell thee work me fearfits;\n",
      "I'll stand in to proclame to the viler\n",
      "Of su\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T12:03:32.765956Z",
     "start_time": "2025-12-19T10:29:09.730506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train\n",
    "for iter in range(25000):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter:4d}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    optimizer.step(xb, yb)"
   ],
   "id": "e5cf940876640ce8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0: train loss 1.3436, val loss 1.5621\n",
      "step  100: train loss 1.3480, val loss 1.5745\n",
      "step  200: train loss 1.3403, val loss 1.5571\n",
      "step  300: train loss 1.3368, val loss 1.5641\n",
      "step  400: train loss 1.3374, val loss 1.5542\n",
      "step  500: train loss 1.3409, val loss 1.5620\n",
      "step  600: train loss 1.3363, val loss 1.5611\n",
      "step  700: train loss 1.3319, val loss 1.5498\n",
      "step  800: train loss 1.3269, val loss 1.5464\n",
      "step  900: train loss 1.3227, val loss 1.5482\n",
      "step 1000: train loss 1.3211, val loss 1.5518\n",
      "step 1100: train loss 1.3258, val loss 1.5445\n",
      "step 1200: train loss 1.3345, val loss 1.5451\n",
      "step 1300: train loss 1.3261, val loss 1.5480\n",
      "step 1400: train loss 1.3310, val loss 1.5441\n",
      "step 1500: train loss 1.3192, val loss 1.5439\n",
      "step 1600: train loss 1.3190, val loss 1.5430\n",
      "step 1700: train loss 1.3136, val loss 1.5381\n",
      "step 1800: train loss 1.3234, val loss 1.5405\n",
      "step 1900: train loss 1.3373, val loss 1.5503\n",
      "step 2000: train loss 1.3054, val loss 1.5383\n",
      "step 2100: train loss 1.3069, val loss 1.5392\n",
      "step 2200: train loss 1.3102, val loss 1.5259\n",
      "step 2300: train loss 1.3245, val loss 1.5482\n",
      "step 2400: train loss 1.3093, val loss 1.5517\n",
      "step 2500: train loss 1.3155, val loss 1.5269\n",
      "step 2600: train loss 1.3075, val loss 1.5345\n",
      "step 2700: train loss 1.2971, val loss 1.5185\n",
      "step 2800: train loss 1.3063, val loss 1.5300\n",
      "step 2900: train loss 1.3053, val loss 1.5398\n",
      "step 3000: train loss 1.3023, val loss 1.5374\n",
      "step 3100: train loss 1.2942, val loss 1.5223\n",
      "step 3200: train loss 1.2996, val loss 1.5323\n",
      "step 3300: train loss 1.3005, val loss 1.5291\n",
      "step 3400: train loss 1.2996, val loss 1.5228\n",
      "step 3500: train loss 1.2967, val loss 1.5201\n",
      "step 3600: train loss 1.2954, val loss 1.5278\n",
      "step 3700: train loss 1.3055, val loss 1.5278\n",
      "step 3800: train loss 1.2921, val loss 1.5209\n",
      "step 3900: train loss 1.3059, val loss 1.5346\n",
      "step 4000: train loss 1.2927, val loss 1.5219\n",
      "step 4100: train loss 1.2826, val loss 1.5268\n",
      "step 4200: train loss 1.2783, val loss 1.5067\n",
      "step 4300: train loss 1.2993, val loss 1.5331\n",
      "step 4400: train loss 1.2830, val loss 1.5193\n",
      "step 4500: train loss 1.2899, val loss 1.5192\n",
      "step 4600: train loss 1.2858, val loss 1.5197\n",
      "step 4700: train loss 1.2769, val loss 1.5177\n",
      "step 4800: train loss 1.2790, val loss 1.5085\n",
      "step 4900: train loss 1.2884, val loss 1.5239\n",
      "step 5000: train loss 1.2886, val loss 1.5202\n",
      "step 5100: train loss 1.2822, val loss 1.5193\n",
      "step 5200: train loss 1.2809, val loss 1.5203\n",
      "step 5300: train loss 1.2778, val loss 1.5150\n",
      "step 5400: train loss 1.2849, val loss 1.5221\n",
      "step 5500: train loss 1.2813, val loss 1.5146\n",
      "step 5600: train loss 1.2738, val loss 1.5122\n",
      "step 5700: train loss 1.2768, val loss 1.5182\n",
      "step 5800: train loss 1.2829, val loss 1.5244\n",
      "step 5900: train loss 1.2709, val loss 1.5089\n",
      "step 6000: train loss 1.2685, val loss 1.5046\n",
      "step 6100: train loss 1.2768, val loss 1.5232\n",
      "step 6200: train loss 1.2715, val loss 1.5160\n",
      "step 6300: train loss 1.2599, val loss 1.4993\n",
      "step 6400: train loss 1.2696, val loss 1.5111\n",
      "step 6500: train loss 1.2604, val loss 1.5065\n",
      "step 6600: train loss 1.2672, val loss 1.5065\n",
      "step 6700: train loss 1.2681, val loss 1.5087\n",
      "step 6800: train loss 1.2598, val loss 1.5048\n",
      "step 6900: train loss 1.2603, val loss 1.5164\n",
      "step 7000: train loss 1.2602, val loss 1.5145\n",
      "step 7100: train loss 1.2666, val loss 1.5087\n",
      "step 7200: train loss 1.2596, val loss 1.5091\n",
      "step 7300: train loss 1.2663, val loss 1.5007\n",
      "step 7400: train loss 1.2582, val loss 1.4987\n",
      "step 7500: train loss 1.2608, val loss 1.4991\n",
      "step 7600: train loss 1.2610, val loss 1.5064\n",
      "step 7700: train loss 1.2585, val loss 1.5002\n",
      "step 7800: train loss 1.2540, val loss 1.4975\n",
      "step 7900: train loss 1.2554, val loss 1.5051\n",
      "step 8000: train loss 1.2612, val loss 1.5062\n",
      "step 8100: train loss 1.2494, val loss 1.5009\n",
      "step 8200: train loss 1.2585, val loss 1.5088\n",
      "step 8300: train loss 1.2509, val loss 1.5024\n",
      "step 8400: train loss 1.2434, val loss 1.4966\n",
      "step 8500: train loss 1.2570, val loss 1.4980\n",
      "step 8600: train loss 1.2459, val loss 1.4947\n",
      "step 8700: train loss 1.2518, val loss 1.4919\n",
      "step 8800: train loss 1.2505, val loss 1.4991\n",
      "step 8900: train loss 1.2446, val loss 1.4975\n",
      "step 9000: train loss 1.2453, val loss 1.4914\n",
      "step 9100: train loss 1.2316, val loss 1.4837\n",
      "step 9200: train loss 1.2418, val loss 1.5041\n",
      "step 9300: train loss 1.2362, val loss 1.4896\n",
      "step 9400: train loss 1.2385, val loss 1.4900\n",
      "step 9500: train loss 1.2381, val loss 1.4873\n",
      "step 9600: train loss 1.2430, val loss 1.4836\n",
      "step 9700: train loss 1.2394, val loss 1.4968\n",
      "step 9800: train loss 1.2389, val loss 1.4901\n",
      "step 9900: train loss 1.2507, val loss 1.4892\n",
      "step 10000: train loss 1.2297, val loss 1.4856\n",
      "step 10100: train loss 1.2312, val loss 1.4810\n",
      "step 10200: train loss 1.2367, val loss 1.4915\n",
      "step 10300: train loss 1.2293, val loss 1.4939\n",
      "step 10400: train loss 1.2263, val loss 1.4799\n",
      "step 10500: train loss 1.2247, val loss 1.4872\n",
      "step 10600: train loss 1.2280, val loss 1.4825\n",
      "step 10700: train loss 1.2384, val loss 1.4928\n",
      "step 10800: train loss 1.2187, val loss 1.4713\n",
      "step 10900: train loss 1.2226, val loss 1.4752\n",
      "step 11000: train loss 1.2343, val loss 1.4827\n",
      "step 11100: train loss 1.2442, val loss 1.4916\n",
      "step 11200: train loss 1.2259, val loss 1.4838\n",
      "step 11300: train loss 1.2292, val loss 1.4921\n",
      "step 11400: train loss 1.2288, val loss 1.4725\n",
      "step 11500: train loss 1.2173, val loss 1.4724\n",
      "step 11600: train loss 1.2317, val loss 1.4890\n",
      "step 11700: train loss 1.2246, val loss 1.4844\n",
      "step 11800: train loss 1.2258, val loss 1.4809\n",
      "step 11900: train loss 1.2202, val loss 1.4779\n",
      "step 12000: train loss 1.2121, val loss 1.4727\n",
      "step 12100: train loss 1.2235, val loss 1.4750\n",
      "step 12200: train loss 1.2188, val loss 1.4806\n",
      "step 12300: train loss 1.2143, val loss 1.4733\n",
      "step 12400: train loss 1.2142, val loss 1.4807\n",
      "step 12500: train loss 1.2148, val loss 1.4735\n",
      "step 12600: train loss 1.2127, val loss 1.4718\n",
      "step 12700: train loss 1.2228, val loss 1.4829\n",
      "step 12800: train loss 1.2181, val loss 1.4750\n",
      "step 12900: train loss 1.2183, val loss 1.4764\n",
      "step 13000: train loss 1.2242, val loss 1.4881\n",
      "step 13100: train loss 1.2191, val loss 1.4677\n",
      "step 13200: train loss 1.2096, val loss 1.4730\n",
      "step 13300: train loss 1.2143, val loss 1.4812\n",
      "step 13400: train loss 1.2138, val loss 1.4752\n",
      "step 13500: train loss 1.2078, val loss 1.4748\n",
      "step 13600: train loss 1.2095, val loss 1.4716\n",
      "step 13700: train loss 1.1987, val loss 1.4588\n",
      "step 13800: train loss 1.2087, val loss 1.4684\n",
      "step 13900: train loss 1.2099, val loss 1.4743\n",
      "step 14000: train loss 1.1990, val loss 1.4736\n",
      "step 14100: train loss 1.2158, val loss 1.4787\n",
      "step 14200: train loss 1.2072, val loss 1.4726\n",
      "step 14300: train loss 1.1949, val loss 1.4655\n",
      "step 14400: train loss 1.2016, val loss 1.4642\n",
      "step 14500: train loss 1.1975, val loss 1.4740\n",
      "step 14600: train loss 1.2011, val loss 1.4731\n",
      "step 14700: train loss 1.1973, val loss 1.4689\n",
      "step 14800: train loss 1.2018, val loss 1.4619\n",
      "step 14900: train loss 1.1892, val loss 1.4592\n",
      "step 15000: train loss 1.2031, val loss 1.4682\n",
      "step 15100: train loss 1.1973, val loss 1.4579\n",
      "step 15200: train loss 1.1953, val loss 1.4688\n",
      "step 15300: train loss 1.1975, val loss 1.4726\n",
      "step 15400: train loss 1.1931, val loss 1.4671\n",
      "step 15500: train loss 1.1859, val loss 1.4654\n",
      "step 15600: train loss 1.2041, val loss 1.4788\n",
      "step 15700: train loss 1.1913, val loss 1.4837\n",
      "step 15800: train loss 1.1794, val loss 1.4550\n",
      "step 15900: train loss 1.2015, val loss 1.4640\n",
      "step 16000: train loss 1.1841, val loss 1.4551\n",
      "step 16100: train loss 1.1929, val loss 1.4716\n",
      "step 16200: train loss 1.1890, val loss 1.4605\n",
      "step 16300: train loss 1.1763, val loss 1.4415\n",
      "step 16400: train loss 1.1864, val loss 1.4720\n",
      "step 16500: train loss 1.1909, val loss 1.4716\n",
      "step 16600: train loss 1.1793, val loss 1.4539\n",
      "step 16700: train loss 1.1896, val loss 1.4703\n",
      "step 16800: train loss 1.1855, val loss 1.4565\n",
      "step 16900: train loss 1.1721, val loss 1.4587\n",
      "step 17000: train loss 1.1737, val loss 1.4575\n",
      "step 17100: train loss 1.1929, val loss 1.4693\n",
      "step 17200: train loss 1.1741, val loss 1.4565\n",
      "step 17300: train loss 1.1734, val loss 1.4584\n",
      "step 17400: train loss 1.1776, val loss 1.4446\n",
      "step 17500: train loss 1.1745, val loss 1.4654\n",
      "step 17600: train loss 1.1815, val loss 1.4556\n",
      "step 17700: train loss 1.1913, val loss 1.4721\n",
      "step 17800: train loss 1.1815, val loss 1.4618\n",
      "step 17900: train loss 1.1904, val loss 1.4632\n",
      "step 18000: train loss 1.1723, val loss 1.4630\n",
      "step 18100: train loss 1.1707, val loss 1.4537\n",
      "step 18200: train loss 1.1819, val loss 1.4652\n",
      "step 18300: train loss 1.1796, val loss 1.4632\n",
      "step 18400: train loss 1.1788, val loss 1.4647\n",
      "step 18500: train loss 1.1725, val loss 1.4536\n",
      "step 18600: train loss 1.1675, val loss 1.4615\n",
      "step 18700: train loss 1.1644, val loss 1.4580\n",
      "step 18800: train loss 1.1549, val loss 1.4452\n",
      "step 18900: train loss 1.1640, val loss 1.4389\n",
      "step 19000: train loss 1.1634, val loss 1.4618\n",
      "step 19100: train loss 1.1624, val loss 1.4491\n",
      "step 19200: train loss 1.1577, val loss 1.4479\n",
      "step 19300: train loss 1.1642, val loss 1.4529\n",
      "step 19400: train loss 1.1532, val loss 1.4391\n",
      "step 19500: train loss 1.1639, val loss 1.4565\n",
      "step 19600: train loss 1.1563, val loss 1.4535\n",
      "step 19700: train loss 1.1645, val loss 1.4445\n",
      "step 19800: train loss 1.1544, val loss 1.4551\n",
      "step 19900: train loss 1.1576, val loss 1.4479\n",
      "step 20000: train loss 1.1527, val loss 1.4507\n",
      "step 20100: train loss 1.1664, val loss 1.4586\n",
      "step 20200: train loss 1.1595, val loss 1.4628\n",
      "step 20300: train loss 1.1560, val loss 1.4551\n",
      "step 20400: train loss 1.1629, val loss 1.4435\n",
      "step 20500: train loss 1.1563, val loss 1.4576\n",
      "step 20600: train loss 1.1494, val loss 1.4497\n",
      "step 20700: train loss 1.1583, val loss 1.4404\n",
      "step 20800: train loss 1.1521, val loss 1.4386\n",
      "step 20900: train loss 1.1515, val loss 1.4498\n",
      "step 21000: train loss 1.1502, val loss 1.4438\n",
      "step 21100: train loss 1.1474, val loss 1.4472\n",
      "step 21200: train loss 1.1518, val loss 1.4461\n",
      "step 21300: train loss 1.1499, val loss 1.4487\n",
      "step 21400: train loss 1.1468, val loss 1.4351\n",
      "step 21500: train loss 1.1511, val loss 1.4477\n",
      "step 21600: train loss 1.1492, val loss 1.4366\n",
      "step 21700: train loss 1.1493, val loss 1.4535\n",
      "step 21800: train loss 1.1490, val loss 1.4606\n",
      "step 21900: train loss 1.1414, val loss 1.4435\n",
      "step 22000: train loss 1.1563, val loss 1.4505\n",
      "step 22100: train loss 1.1412, val loss 1.4394\n",
      "step 22200: train loss 1.1423, val loss 1.4419\n",
      "step 22300: train loss 1.1386, val loss 1.4502\n",
      "step 22400: train loss 1.1393, val loss 1.4290\n",
      "step 22500: train loss 1.1411, val loss 1.4432\n",
      "step 22600: train loss 1.1334, val loss 1.4455\n",
      "step 22700: train loss 1.1357, val loss 1.4318\n",
      "step 22800: train loss 1.1354, val loss 1.4476\n",
      "step 22900: train loss 1.1337, val loss 1.4239\n",
      "step 23000: train loss 1.1367, val loss 1.4261\n",
      "step 23100: train loss 1.1478, val loss 1.4425\n",
      "step 23200: train loss 1.1452, val loss 1.4386\n",
      "step 23300: train loss 1.1354, val loss 1.4257\n",
      "step 23400: train loss 1.1420, val loss 1.4434\n",
      "step 23500: train loss 1.1407, val loss 1.4345\n",
      "step 23600: train loss 1.1368, val loss 1.4409\n",
      "step 23700: train loss 1.1268, val loss 1.4236\n",
      "step 23800: train loss 1.1251, val loss 1.4365\n",
      "step 23900: train loss 1.1398, val loss 1.4379\n",
      "step 24000: train loss 1.1340, val loss 1.4439\n",
      "step 24100: train loss 1.1353, val loss 1.4380\n",
      "step 24200: train loss 1.1114, val loss 1.4160\n",
      "step 24300: train loss 1.1332, val loss 1.4447\n",
      "step 24400: train loss 1.1176, val loss 1.4318\n",
      "step 24500: train loss 1.1323, val loss 1.4387\n",
      "step 24600: train loss 1.1312, val loss 1.4373\n",
      "step 24700: train loss 1.1292, val loss 1.4461\n",
      "step 24800: train loss 1.1248, val loss 1.4262\n",
      "step 24900: train loss 1.1272, val loss 1.4390\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T17:43:04.921317Z",
     "start_time": "2025-12-19T17:42:57.302827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate text priming\n",
    "def generate_with_priming(network, prompt=\"\", max_new_tokens=500, temperature=1.0):\n",
    "    prime_text = \"\"\"First Citizen:\n",
    "Before we proceed any further, hear me speak.\n",
    "\n",
    "All:\n",
    "Speak, speak.\n",
    "\n",
    "First Citizen:\n",
    "You are all resolved rather to die than to famish?\n",
    "\n",
    "All:\n",
    "Resolved. Resolved.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    full_start = prime_text + prompt\n",
    "    generated = generate_text(network, full_start, max_new_tokens, temperature)\n",
    "    return generated[len(prompt):]\n",
    "\n",
    "\n",
    "start_texts = [\n",
    "    \"First Citizen:\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"The \",\n",
    "]\n",
    "\n",
    "for start in start_texts:\n",
    "    generated = generate_with_priming(network, prompt=start)\n",
    "    print(f\"Starting with: {repr(start)}\")\n",
    "    print(generated)\n",
    "    print(\"-\" * 80)"
   ],
   "id": "c0067cef1831f4b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with: 'First Citizen:\\n'\n",
      "ttmeotC ie:ri rie.ti.trii horot\n",
      "to!h\n",
      "eueeyh\n",
      "\n",
      "tn Soe the pointitors mar,\n",
      "wretchieth with thou this pity anon:\n",
      "It virtue hither cway him to two a not.\n",
      "What doptriging or sounsure earth.\n",
      "\n",
      "CORIOLANUS:\n",
      "It is repiled with Unclimity too.\n",
      "\n",
      "First Still:\n",
      "How make you,\n",
      "Awiltier frightily grace in to blood with to where.\n",
      "\n",
      "BENVOLIO:\n",
      "Why, 'tis is often too this wistcharged\n",
      "You pleasince in her brother; till them it no art\n",
      "crown out.\n",
      "\n",
      "COMINIUS:\n",
      "Grovello thee trust of me?\n",
      "\n",
      "Post:\n",
      "Ay, tthat thou th\n",
      "--------------------------------------------------------------------------------\n",
      "Starting with: '\\n\\n'\n",
      "trsxepC ty..'vttS:oost.ttept-eiyM e \n",
      "PUittoo\n",
      "t : e e--te tril\n",
      "ott hpa neenomrous powern, think will be prodilege taonce.\n",
      "\n",
      "Shild Citizen:\n",
      "The drew sparting--off; to a who should not held is mort.\n",
      "\n",
      "VOLUMHNIUS:\n",
      "Away, 'tis on!\n",
      "\n",
      "CORIOLANUS:\n",
      "Cannot Coriol--\n",
      "Common.\n",
      "\n",
      "Cate-gentleman:\n",
      "I'll have a littering he to much mockerouse.\n",
      "\n",
      "MONTAGUE:\n",
      "'Tis arcalmony hear to too much not, the power!\n",
      "\n",
      "First Citizen:\n",
      "Heart your in cloud inmattable.\n",
      "\n",
      "Third Romeo instulture.\n",
      "\n",
      "CORIOLANUS:\n",
      "From, his. Let's the pititions:\n",
      "--------------------------------------------------------------------------------\n",
      "Starting with: 'The '\n",
      "etittbe;otntitetlt.\n",
      " attveittti thkaitfetr.mse\n",
      "wrenioosnti\n",
      ",to re;oimmnidess man, in profit of thine: forth\n",
      "hither trusting of Londonstight them, be\n",
      "give it of Silenity and not fall the hour.\n",
      "\n",
      "VOLUMNIA:\n",
      "I that try poor him stattion!\n",
      "\n",
      "BRUTUSCIO:\n",
      "O, nobless i't.\n",
      "\n",
      "CAMILLIUS:\n",
      "I will tribundly, lord. I am it would tremble in any\n",
      "Without you haverts in to slew me to be true!\n",
      "not rodDUmand, with he would bring more this.\n",
      "\n",
      "CORIOLANUS:\n",
      "You twill it.\n",
      "\n",
      "MENENIUS:\n",
      "Brother, I take your virtue. How is form\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
