{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-19T07:41:49.423152Z",
     "start_time": "2025-12-19T07:41:29.987556Z"
    }
   },
   "source": [
    "import math\n",
    "\n",
    "import mlx.core as mx\n",
    "import numpy as np\n",
    "from mlx import nn\n",
    "\n",
    "from softgrad import Network\n",
    "from softgrad.function.activation import Relu, Softmax, softmax\n",
    "from softgrad.function.core import Add, Concatenate\n",
    "from softgrad.function.loss import CrossEntropyLoss, sequence_ce_loss\n",
    "from softgrad.layer.attn import CausalSelfAttentionHead\n",
    "from softgrad.layer.core import Parallel, Embedding, Sequential, Linear, Residual, Activation\n",
    "from softgrad.layer.norm import LayerNorm\n",
    "from softgrad.layer.shim import MLX\n",
    "from softgrad.layer.transform.PositionIndices import PositionIndices\n",
    "from softgrad.optim import SGD\n",
    "\n",
    "\n",
    "class MLXCausalSelfAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "\n",
    "        self.n_heads = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.causal_mask = MLXCausalSelfAttention.create_additive_causal_mask(block_size, dtype=mx.bfloat16)\n",
    "\n",
    "        self.query_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.key_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.value_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.out_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # calculate query, key, value for all heads\n",
    "        q = self.query_proj(x) # (B, T, C) -> (B, T, C)\n",
    "        k = self.key_proj(x) # (B, T, C) -> (B, T, C)\n",
    "        v = self.value_proj(x) # (B, T, C) -> (B, T, C)\n",
    "\n",
    "        # reshape query, key, value to batch over n_batches x n_heads\n",
    "        #   - this way we can compute attention for all heads at once (i.e. multi-head attention) with a single matrix multiply\n",
    "        #   - nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        q = mx.unflatten(q, -1, (self.n_heads, -1)).transpose(0, 2, 1, 3) # (B, T, C) -> (B, T, nh, hs) -> (B, nh, T, hs)\n",
    "        k = mx.unflatten(k, -1, (self.n_heads, -1)).transpose(0, 2, 1, 3) # (B, T, C) -> (B, T, nh, hs) -> (B, nh, T, hs)\n",
    "        v = mx.unflatten(v, -1, (self.n_heads, -1)).transpose(0, 2, 1, 3) # (B, T, C) -> (B, T, nh, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        # causal flash attention\n",
    "        scale = math.sqrt(1 / q.shape[-1])\n",
    "        output = mx.fast.scaled_dot_product_attention(q, k, v, scale=scale, mask=self.causal_mask[:T, :T]) # 3x(B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        # re-assemble all head outputs side by side and project out\n",
    "        output = output.transpose(0, 2, 1, 3).flatten(-2, -1) # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, C)\n",
    "        return self.out_proj(output) # (B, T, C) -> (B, T, C)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_additive_causal_mask(N: int, dtype = mx.float32):\n",
    "        indices = mx.arange(N)\n",
    "        mask = indices[:, None] < indices[None]\n",
    "        mask = mask.astype(dtype) * mx.finfo(dtype).min\n",
    "        return mask\n",
    "\n",
    "\n",
    "class FeedForward(Sequential):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__([\n",
    "            Linear(4 * n_embd),\n",
    "            Activation(Relu()),\n",
    "            Linear(n_embd)\n",
    "        ])\n",
    "\n",
    "\n",
    "class MultiHeadAttention(Sequential):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__([\n",
    "            Parallel(\n",
    "                [CausalSelfAttentionHead(n_embd, head_size, block_size) for _ in range(num_heads)]  # heads\n",
    "            , Concatenate()),\n",
    "            Linear(n_embd)  # projection\n",
    "        ])\n",
    "\n",
    "\n",
    "class TransformerBlock(Sequential):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__([\n",
    "            # communication\n",
    "            Residual(Sequential([\n",
    "                LayerNorm(),\n",
    "                MultiHeadAttention(n_head, n_embd // n_head)\n",
    "                # MLX(MLXCausalSelfAttention())\n",
    "            ])),\n",
    "            # computation\n",
    "            Residual(Sequential([\n",
    "                LayerNorm(),\n",
    "                FeedForward(n_embd)\n",
    "            ]))\n",
    "        ])\n",
    "\n",
    "\n",
    "mx.random.seed(1337)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Hyperparameters\n",
    "# ----------------------------------------------------------------------------------\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "max_iters = 25000\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-2\n",
    "eval_iters = 50\n",
    "n_embd = 128            # each token -> 128\n",
    "n_head = 4              # 4 heads -> 32\n",
    "n_layer = 2             # 2 transformer blocks\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Load Dataset\n",
    "# ----------------------------------------------------------------------------------\n",
    "with open('rsc/tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = mx.array(encode(text))\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = mx.random.randint(0, len(data_split) - block_size, (batch_size,))\n",
    "    x = mx.stack([data_split[int(i):int(i) + block_size] for i in ix])\n",
    "    y = mx.stack([data_split[int(i) + 1:int(i) + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def generate_text(network, start_text=\"\", max_new_tokens=500, temperature=1.0, top_k=None):\n",
    "    if start_text:\n",
    "        context = encode(start_text)\n",
    "    else:\n",
    "        context = [0]\n",
    "\n",
    "    context = list(context)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        if len(context) < block_size:\n",
    "            context_padded = [0] * (block_size - len(context)) + context  # pad 0s on the left\n",
    "        else:\n",
    "            context_padded = context[-block_size:]  # take as much as we can fit into context\n",
    "\n",
    "        context_array = mx.array(context_padded)[None, :]  # (1, block_size)\n",
    "        logits = network.forward(context_array, save_ctx=False)  # (1, block_size, vocab_size)\n",
    "\n",
    "        if len(context) < block_size:\n",
    "            logits = logits[:, len(context) - 1, :]  # (1, vocab_size)\n",
    "        else:\n",
    "            logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "\n",
    "        logits = logits / temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_values = mx.sort(logits[0])[-top_k:]\n",
    "            threshold = top_values[0]\n",
    "            logits_filtered = mx.where(logits[0] >= threshold, logits[0], float('-inf'))\n",
    "            logits = logits_filtered[None, :]\n",
    "\n",
    "        probs = mx.softmax(logits, axis=-1) # convert to probabilities\n",
    "        idx_next = mx.random.categorical(mx.log(probs[0]), num_samples=1) # sample from distribution\n",
    "        context.append(int(idx_next[0]))\n",
    "\n",
    "    if start_text:\n",
    "        generated_tokens = context[len(encode(start_text)):]\n",
    "    else:\n",
    "        generated_tokens = context[1:]\n",
    "\n",
    "    return decode(generated_tokens)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Setup Network\n",
    "# ----------------------------------------------------------------------------------\n",
    "print(\"Setting up network...\")\n",
    "network = Network(input_shape=(block_size,))\n",
    "network.add_layer(Parallel([\n",
    "    Embedding(vocab_size, n_embd),  # Semantic encoding\n",
    "    Sequential([\n",
    "        PositionIndices(),\n",
    "        Embedding(block_size, n_embd)  # Positional encoding\n",
    "    ])\n",
    "], Add()))\n",
    "network.add_layer(Sequential(\n",
    "    [TransformerBlock(n_embd, n_head) for _ in range(n_layer)]  # transformer blocks\n",
    "))\n",
    "network.add_layer(LayerNorm())\n",
    "network.add_layer(Linear(vocab_size))  # LLM head\n",
    "\n",
    "print(\"Setting up optimizer...\")\n",
    "optimizer = SGD(eta=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer.bind_loss_fn(sequence_ce_loss)\n",
    "optimizer.bind_network(network)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Evaluation function\n",
    "# ----------------------------------------------------------------------------------\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = []\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "\n",
    "            # forward pass\n",
    "            logits = network.forward(X, save_ctx=False)\n",
    "\n",
    "            # compute loss\n",
    "            loss_per_token = sequence_ce_loss.apply(logits, Y)\n",
    "            mean_loss = mx.mean(loss_per_token)\n",
    "\n",
    "            losses.append(mean_loss.item())\n",
    "\n",
    "        out[split] = np.mean(losses)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Train Loop\n",
    "# ----------------------------------------------------------------------------------\n",
    "print(\"-\" * 50)\n",
    "print(\"\\nTraining...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter:4d}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    optimizer.step(xb, yb)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# FINAL EVALUATION\n",
    "# ----------------------------------------------------------------------------------\n",
    "losses = estimate_loss()\n",
    "print(f\"Final: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Generate\n",
    "# ----------------------------------------------------------------------------------\n",
    "prompts = [\n",
    "    \"ROMEO:\",\n",
    "    \"To be or not to be\",\n",
    "    \"First Citizen:\\n\",\n",
    "    \"The king\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"-\" * 40)\n",
    "    generated = generate_text(\n",
    "        network,\n",
    "        start_text=prompt,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.8,\n",
    "        top_k=40\n",
    "    )\n",
    "    print(prompt + generated)\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up network...\n",
      "Setting up optimizer...\n",
      "\n",
      "Training...\n",
      "============================================================\n",
      "step    0: train loss 4.2880, val loss 4.2829\n",
      "step  100: train loss 3.3150, val loss 3.3413\n",
      "step  200: train loss 3.2114, val loss 3.2473\n",
      "step  300: train loss 3.0906, val loss 3.1287\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 268\u001B[39m\n\u001B[32m    265\u001B[39m     xb, yb = get_batch(\u001B[33m'\u001B[39m\u001B[33mtrain\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    267\u001B[39m     \u001B[38;5;66;03m# Optimization step (forward + backward + update)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m268\u001B[39m     \u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43myb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    270\u001B[39m \u001B[38;5;66;03m# ============================================================================\u001B[39;00m\n\u001B[32m    271\u001B[39m \u001B[38;5;66;03m# FINAL EVALUATION\u001B[39;00m\n\u001B[32m    272\u001B[39m \u001B[38;5;66;03m# ============================================================================\u001B[39;00m\n\u001B[32m    273\u001B[39m losses = estimate_loss()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpine/_ai_/softgrad/src/softgrad/optim/SGD.py:19\u001B[39m, in \u001B[36mSGD.step\u001B[39m\u001B[34m(self, x, y)\u001B[39m\n\u001B[32m     17\u001B[39m grad = \u001B[38;5;28mself\u001B[39m.loss_fn.derivative(y_pred, y)\n\u001B[32m     18\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(\u001B[38;5;28mself\u001B[39m.network.layers):\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m     grad = \u001B[43mlayer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[38;5;66;03m# update layers\u001B[39;00m\n\u001B[32m     22\u001B[39m trainable_layers = []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpine/_ai_/softgrad/src/softgrad/layer/Layer.py:28\u001B[39m, in \u001B[36mLayer.backward\u001B[39m\u001B[34m(self, dx_out, save_ctx)\u001B[39m\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mbackward\u001B[39m(\u001B[38;5;28mself\u001B[39m, dx_out: mx.array, save_ctx=\u001B[38;5;28;01mTrue\u001B[39;00m) -> mx.array:\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m     dx_in = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdx_out\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     29\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m save_ctx:\n\u001B[32m     30\u001B[39m         \u001B[38;5;28mself\u001B[39m.ctx.dx_out = dx_out\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpine/_ai_/softgrad/src/softgrad/layer/core/Parallel.py:61\u001B[39m, in \u001B[36mParallel._backward\u001B[39m\u001B[34m(self, dx_out)\u001B[39m\n\u001B[32m     59\u001B[39m dx_ins = []\n\u001B[32m     60\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m.layers):\n\u001B[32m---> \u001B[39m\u001B[32m61\u001B[39m     dx_in = \u001B[43mlayer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43md_outputs\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     62\u001B[39m     dx_ins.append(dx_in)\n\u001B[32m     64\u001B[39m \u001B[38;5;66;03m# Sum gradients from all branches (they all came from the same input)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpine/_ai_/softgrad/src/softgrad/layer/Layer.py:28\u001B[39m, in \u001B[36mLayer.backward\u001B[39m\u001B[34m(self, dx_out, save_ctx)\u001B[39m\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mbackward\u001B[39m(\u001B[38;5;28mself\u001B[39m, dx_out: mx.array, save_ctx=\u001B[38;5;28;01mTrue\u001B[39;00m) -> mx.array:\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m     dx_in = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdx_out\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     29\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m save_ctx:\n\u001B[32m     30\u001B[39m         \u001B[38;5;28mself\u001B[39m.ctx.dx_out = dx_out\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpine/_ai_/softgrad/src/softgrad/layer/core/Sequential.py:32\u001B[39m, in \u001B[36mSequential._backward\u001B[39m\u001B[34m(self, dx_out)\u001B[39m\n\u001B[32m     30\u001B[39m dx = dx_out\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(\u001B[38;5;28mself\u001B[39m.layers):\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m     dx = \u001B[43mlayer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m dx\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpine/_ai_/softgrad/src/softgrad/layer/Layer.py:28\u001B[39m, in \u001B[36mLayer.backward\u001B[39m\u001B[34m(self, dx_out, save_ctx)\u001B[39m\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mbackward\u001B[39m(\u001B[38;5;28mself\u001B[39m, dx_out: mx.array, save_ctx=\u001B[38;5;28;01mTrue\u001B[39;00m) -> mx.array:\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m     dx_in = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdx_out\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     29\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m save_ctx:\n\u001B[32m     30\u001B[39m         \u001B[38;5;28mself\u001B[39m.ctx.dx_out = dx_out\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpine/_ai_/softgrad/src/softgrad/layer/core/Embedding.py:42\u001B[39m, in \u001B[36mEmbedding._backward\u001B[39m\u001B[34m(self, dx_out)\u001B[39m\n\u001B[32m     38\u001B[39m     mask = (indices_flat == idx).astype(mx.float32)\n\u001B[32m     40\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m mx.sum(mask) > \u001B[32m0\u001B[39m:\n\u001B[32m     41\u001B[39m         \u001B[38;5;66;03m# Sum gradients for all occurrences of this index\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m         grad_for_idx = \u001B[43mmx\u001B[49m\u001B[43m.\u001B[49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     43\u001B[39m \u001B[43m            \u001B[49m\u001B[43mdx_out_flat\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnewaxis\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     44\u001B[39m \u001B[43m            \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\n\u001B[32m     45\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     46\u001B[39m         \u001B[38;5;28mself\u001B[39m.params[\u001B[33m\"\u001B[39m\u001B[33mdembeddings\u001B[39m\u001B[33m\"\u001B[39m][idx] += grad_for_idx\n\u001B[32m     48\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m mx.zeros(indices.shape, dtype=mx.float32)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T07:41:04.332652Z",
     "start_time": "2025-12-19T07:36:11.358480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train\n",
    "for iter in range(5000):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter:4d}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    optimizer.step(xb, yb)"
   ],
   "id": "87b36039a4c746b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0: train loss 1.8133, val loss 1.8961\n",
      "step  100: train loss 1.7883, val loss 1.8877\n",
      "step  200: train loss 1.7873, val loss 1.8709\n",
      "step  300: train loss 1.7886, val loss 1.8844\n",
      "step  400: train loss 1.7828, val loss 1.8800\n",
      "step  500: train loss 1.7825, val loss 1.8822\n",
      "step  600: train loss 1.7738, val loss 1.8754\n",
      "step  700: train loss 1.7719, val loss 1.8621\n",
      "step  800: train loss 1.7772, val loss 1.8685\n",
      "step  900: train loss 1.7639, val loss 1.8584\n",
      "step 1000: train loss 1.7837, val loss 1.8741\n",
      "step 1100: train loss 1.7739, val loss 1.8651\n",
      "step 1200: train loss 1.7684, val loss 1.8606\n",
      "step 1300: train loss 1.7681, val loss 1.8633\n",
      "step 1400: train loss 1.7656, val loss 1.8584\n",
      "step 1500: train loss 1.7602, val loss 1.8513\n",
      "step 1600: train loss 1.7486, val loss 1.8526\n",
      "step 1700: train loss 1.7531, val loss 1.8616\n",
      "step 1800: train loss 1.7606, val loss 1.8697\n",
      "step 1900: train loss 1.7546, val loss 1.8424\n",
      "step 2000: train loss 1.7388, val loss 1.8450\n",
      "step 2100: train loss 1.7478, val loss 1.8562\n",
      "step 2200: train loss 1.7378, val loss 1.8347\n",
      "step 2300: train loss 1.7417, val loss 1.8460\n",
      "step 2400: train loss 1.7370, val loss 1.8438\n",
      "step 2500: train loss 1.7366, val loss 1.8461\n",
      "step 2600: train loss 1.7257, val loss 1.8398\n",
      "step 2700: train loss 1.7389, val loss 1.8307\n",
      "step 2800: train loss 1.7237, val loss 1.8424\n",
      "step 2900: train loss 1.7494, val loss 1.8543\n",
      "step 3000: train loss 1.7237, val loss 1.8349\n",
      "step 3100: train loss 1.7241, val loss 1.8389\n",
      "step 3200: train loss 1.7274, val loss 1.8318\n",
      "step 3300: train loss 1.7246, val loss 1.8264\n",
      "step 3400: train loss 1.7192, val loss 1.8422\n",
      "step 3500: train loss 1.7211, val loss 1.8406\n",
      "step 3600: train loss 1.7175, val loss 1.8343\n",
      "step 3700: train loss 1.7020, val loss 1.8255\n",
      "step 3800: train loss 1.7180, val loss 1.8374\n",
      "step 3900: train loss 1.7190, val loss 1.8356\n",
      "step 4000: train loss 1.7127, val loss 1.8350\n",
      "step 4100: train loss 1.7143, val loss 1.8340\n",
      "step 4200: train loss 1.7114, val loss 1.8275\n",
      "step 4300: train loss 1.7026, val loss 1.8260\n",
      "step 4400: train loss 1.6964, val loss 1.8193\n",
      "step 4500: train loss 1.7037, val loss 1.8230\n",
      "step 4600: train loss 1.6899, val loss 1.8116\n",
      "step 4700: train loss 1.6973, val loss 1.8203\n",
      "step 4800: train loss 1.7006, val loss 1.8177\n",
      "step 4900: train loss 1.7080, val loss 1.8231\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T07:41:05.248809Z",
     "start_time": "2025-12-19T07:41:04.333417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate\n",
    "start_texts = [\n",
    "    \"First Citizen:\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"The \",\n",
    "]\n",
    "\n",
    "for start in start_texts:\n",
    "    generated = generate_text(network, start_text=start, max_new_tokens=200)\n",
    "    print(f\"Starting with: {repr(start)}\")\n",
    "    print(generated)\n",
    "    print(\"-\" * 80)"
   ],
   "id": "55f751086280c5d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with: 'First Citizen:\\n'\n",
      "ICIHAIIITiwhiIhITMIATIISICkOYYhhCFiTTBWiIISAEMJREMSTCipTOsvW::ei:in:ns :yIrhitsIyW:ii:ii idii ie:  tke nt\n",
      "tihwh  zectringielth word I spriselflike grat\n",
      "ThinD his kin hath bendsech nill onswemby.\n",
      "AUT-l\n",
      "--------------------------------------------------------------------------------\n",
      "Starting with: '\\n\\n'\n",
      "hiiiIIIIEhhhihIBISIASwiIIIIIAIBItiWIhLWLIXWeSiEHITIIYPGOGSEICISTATUCienaTHy:c;j,idwiinaaI:isinnh:y:  :;  hi \n",
      " :a,  aiss ur mnoase waas hatngess\n",
      "I gath I all, good try ackiffe,\n",
      "So I surd, shald dries\n",
      "T\n",
      "--------------------------------------------------------------------------------\n",
      "Starting with: 'The '\n",
      "hNIA:IhIIIwWTiIiTIAITIIhIYSIIIhIwCNCRETCMSGHI:IIIgJIuApTISCVRIIILiSTIfaIAni:w:hUOciIiIn rI: iOntI:t ,ikhshh w  \n",
      "sc  iaiobkne anyI\n",
      "atERncest thy have yenmmoms'Oorok the not that caman\n",
      "Musfienfole it su\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "564d10ad932f9249"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
