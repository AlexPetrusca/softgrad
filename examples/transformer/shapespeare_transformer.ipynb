{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-19T20:29:58.279061Z",
     "start_time": "2025-12-19T20:01:18.600451Z"
    }
   },
   "source": [
    "import math\n",
    "\n",
    "import mlx.core as mx\n",
    "import numpy as np\n",
    "from mlx import nn\n",
    "\n",
    "from softgrad import Network\n",
    "from softgrad.function.activation import Relu, Softmax, softmax\n",
    "from softgrad.function.core import Add, Concatenate\n",
    "from softgrad.function.loss import CrossEntropyLoss, sequence_ce_loss\n",
    "from softgrad.layer.attn import CausalSelfAttentionHead\n",
    "from softgrad.layer.core import Parallel, Embedding, Sequential, Linear, Residual, Activation\n",
    "from softgrad.layer.norm import LayerNorm\n",
    "from softgrad.layer.transform.PositionIndices import PositionIndices\n",
    "from softgrad.optim import SGD\n",
    "\n",
    "\n",
    "class FeedForward(Sequential):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__([\n",
    "            Linear(4 * n_embd),\n",
    "            Activation(Relu()),\n",
    "            Linear(n_embd)\n",
    "        ])\n",
    "\n",
    "\n",
    "class MultiHeadAttention(Sequential):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__([\n",
    "            Parallel(\n",
    "                [CausalSelfAttentionHead(n_embd, head_size, block_size) for _ in range(num_heads)]  # heads\n",
    "            , Concatenate()),\n",
    "            Linear(n_embd)  # projection\n",
    "        ])\n",
    "\n",
    "\n",
    "class TransformerBlock(Sequential):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__([\n",
    "            # communication\n",
    "            Residual(Sequential([\n",
    "                LayerNorm(),\n",
    "                MultiHeadAttention(n_head, n_embd // n_head)\n",
    "            ])),\n",
    "            # computation\n",
    "            Residual(Sequential([\n",
    "                LayerNorm(),\n",
    "                FeedForward(n_embd)\n",
    "            ]))\n",
    "        ])\n",
    "\n",
    "\n",
    "mx.random.seed(1337)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Hyperparameters\n",
    "# ----------------------------------------------------------------------------------\n",
    "batch_size = 32\n",
    "block_size = 256\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-2\n",
    "eval_iters = 50\n",
    "n_embd = 256\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Load Dataset\n",
    "# ----------------------------------------------------------------------------------\n",
    "with open('rsc/tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = mx.array(encode(text))\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = mx.random.randint(0, len(data_split) - block_size, (batch_size,))\n",
    "    x = mx.stack([data_split[int(i):int(i) + block_size] for i in ix])\n",
    "    y = mx.stack([data_split[int(i) + 1:int(i) + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def generate_text(network, start_text=\"\", max_new_tokens=500, temperature=1.0, top_k=None):\n",
    "    if start_text:\n",
    "        context = encode(start_text)\n",
    "    else:\n",
    "        context = [0]\n",
    "\n",
    "    context = list(context)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        if len(context) < block_size:\n",
    "            context_padded = [0] * (block_size - len(context)) + context  # pad 0s on the left\n",
    "        else:\n",
    "            context_padded = context[-block_size:]  # take as much as we can fit into context\n",
    "\n",
    "        context_array = mx.array(context_padded)[None, :]  # (1, block_size)\n",
    "        logits = network.forward(context_array, save_ctx=False)  # (1, block_size, vocab_size)\n",
    "\n",
    "        if len(context) < block_size:\n",
    "            logits = logits[:, len(context) - 1, :]  # (1, vocab_size)\n",
    "        else:\n",
    "            logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "\n",
    "        logits = logits / temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_values = mx.sort(logits[0])[-top_k:]\n",
    "            threshold = top_values[0]\n",
    "            logits_filtered = mx.where(logits[0] >= threshold, logits[0], float('-inf'))\n",
    "            logits = logits_filtered[None, :]\n",
    "\n",
    "        probs = mx.softmax(logits, axis=-1) # convert to probabilities\n",
    "        idx_next = mx.random.categorical(mx.log(probs[0]), num_samples=1) # sample from distribution\n",
    "        context.append(int(idx_next[0]))\n",
    "\n",
    "    if start_text:\n",
    "        generated_tokens = context[len(encode(start_text)):]\n",
    "    else:\n",
    "        generated_tokens = context[1:]\n",
    "\n",
    "    return decode(generated_tokens)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Setup Network\n",
    "# ----------------------------------------------------------------------------------\n",
    "network = Network(input_shape=(block_size,))\n",
    "network.add_layer(Parallel([\n",
    "    Embedding(vocab_size, n_embd),  # Semantic encoding\n",
    "    Sequential([\n",
    "        PositionIndices(),\n",
    "        Embedding(block_size, n_embd)  # Positional encoding\n",
    "    ])\n",
    "], Add()))\n",
    "network.add_layer(Sequential(\n",
    "    [TransformerBlock(n_embd, n_head) for _ in range(n_layer)]  # transformer blocks\n",
    "))\n",
    "network.add_layer(LayerNorm())\n",
    "network.add_layer(Linear(vocab_size))  # LLM head\n",
    "\n",
    "optimizer = SGD(eta=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer.bind_loss_fn(sequence_ce_loss)\n",
    "optimizer.bind_network(network)\n",
    "\n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = []\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "\n",
    "            # forward pass\n",
    "            logits = network.forward(X, save_ctx=False)\n",
    "\n",
    "            # compute loss\n",
    "            loss_per_token = sequence_ce_loss.apply(logits, Y)\n",
    "            mean_loss = mx.mean(loss_per_token)\n",
    "\n",
    "            losses.append(mean_loss.item())\n",
    "\n",
    "        out[split] = np.mean(losses)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Train Loop\n",
    "# ----------------------------------------------------------------------------------\n",
    "print(\"Training...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter:4d}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    optimizer.step(xb, yb)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Final Evaluation\n",
    "# ----------------------------------------------------------------------------------\n",
    "losses = estimate_loss()\n",
    "print(f\"Final: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "prompts = [\n",
    "    \"ROMEO:\",\n",
    "    \"To be or not to be\",\n",
    "    \"First Citizen:\\n\",\n",
    "    \"The king\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(\"-\" * 40)\n",
    "    generated = generate_text(\n",
    "        network,\n",
    "        start_text=prompt,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.8,\n",
    "        top_k=40\n",
    "    )\n",
    "    print(prompt + generated)\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "--------------------------------------------------\n",
      "step    0: train loss 4.3638, val loss 4.3552\n",
      "step  100: train loss 3.2845, val loss 3.3251\n",
      "step  200: train loss 3.1693, val loss 3.2127\n",
      "step  300: train loss 2.9192, val loss 2.9483\n",
      "step  400: train loss 2.7731, val loss 2.7905\n",
      "step  500: train loss 2.6946, val loss 2.7135\n",
      "step  600: train loss 2.6488, val loss 2.6597\n",
      "step  700: train loss 2.6190, val loss 2.6262\n",
      "step  800: train loss 2.5955, val loss 2.6084\n",
      "step  900: train loss 2.5808, val loss 2.5860\n",
      "step 1000: train loss 2.5655, val loss 2.5707\n",
      "step 1100: train loss 2.5494, val loss 2.5558\n",
      "step 1200: train loss 2.5418, val loss 2.5430\n",
      "step 1300: train loss 2.5304, val loss 2.5376\n",
      "step 1400: train loss 2.5214, val loss 2.5270\n",
      "step 1500: train loss 2.5118, val loss 2.5235\n",
      "step 1600: train loss 2.5060, val loss 2.5156\n",
      "step 1700: train loss 2.4980, val loss 2.5044\n",
      "step 1800: train loss 2.4966, val loss 2.5014\n",
      "step 1900: train loss 2.4848, val loss 2.4934\n",
      "step 2000: train loss 2.4827, val loss 2.4896\n",
      "step 2100: train loss 2.4764, val loss 2.4818\n",
      "step 2200: train loss 2.4720, val loss 2.4810\n",
      "step 2300: train loss 2.4688, val loss 2.4778\n",
      "step 2400: train loss 2.4673, val loss 2.4747\n",
      "step 2500: train loss 2.4595, val loss 2.4679\n",
      "step 2600: train loss 2.4565, val loss 2.4703\n",
      "step 2700: train loss 2.4538, val loss 2.4581\n",
      "step 2800: train loss 2.4462, val loss 2.4557\n",
      "step 2900: train loss 2.4424, val loss 2.4587\n",
      "step 3000: train loss 2.4425, val loss 2.4536\n",
      "step 3100: train loss 2.4392, val loss 2.4467\n",
      "step 3200: train loss 2.4357, val loss 2.4511\n",
      "step 3300: train loss 2.4327, val loss 2.4466\n",
      "step 3400: train loss 2.4253, val loss 2.4429\n",
      "step 3500: train loss 2.4272, val loss 2.4433\n",
      "step 3600: train loss 2.4177, val loss 2.4322\n",
      "step 3700: train loss 2.4170, val loss 2.4339\n",
      "step 3800: train loss 2.4104, val loss 2.4262\n",
      "step 3900: train loss 2.4084, val loss 2.4276\n",
      "step 4000: train loss 2.4086, val loss 2.4222\n",
      "step 4100: train loss 2.4028, val loss 2.4198\n",
      "step 4200: train loss 2.3997, val loss 2.4177\n",
      "step 4300: train loss 2.4003, val loss 2.4191\n",
      "step 4400: train loss 2.3968, val loss 2.4123\n",
      "step 4500: train loss 2.3921, val loss 2.4095\n",
      "step 4600: train loss 2.3900, val loss 2.4105\n",
      "step 4700: train loss 2.3890, val loss 2.4100\n",
      "step 4800: train loss 2.3860, val loss 2.4037\n",
      "step 4900: train loss 2.3777, val loss 2.3932\n",
      "Final: train loss 2.3773, val loss 2.3995\n",
      "\n",
      "Prompt: 'ROMEO:'\n",
      "----------------------------------------\n",
      "ROMEO: rT\n",
      "HOCSOTTSO!LICI!ANUDGLOSNSQKASNLLSSXSNFHRUENILSJSSSGSTNqC'NGLGUUNWNNGS$CESSCTMD!SDBTLK3CNWN3ITLSUSLDCLSHIKTLIMIGNSLSPSVQLwSRSrIONUCQS.TL' ItLQCeVG \n",
      "\n",
      "\n",
      "Prompt: 'To be or not to be'\n",
      "----------------------------------------\n",
      "To be or not to beSSESUtSASC3S!OGNU'SEYINCGJNWNISSDSUT tLYNSLITBSOIILCUSrLqAFIUSGSCUNIS'SLS3DIxNTISCCCKCUSSCSECCGITSTCLNSTNN\n",
      "UCCNUSSANCULCN:ENLLUNENRiQC QQS:C':CLIES SG\n",
      "\n",
      "\n",
      "Prompt: 'First Citizen:\n",
      "'\n",
      "----------------------------------------\n",
      "First Citizen:\n",
      "NCINIITLSWWNINOLNSTGIPSCSILOOS3FCSLGU GCMLCSSSOEGSQANXKNSUDTSSWNTCGCCISASqLTOGSCASCSTLESULHINFQHCCLRIKCASSSLNSDCECtISNUU\n",
      "L?T'UUE\n",
      "$EISIUZ ILAS I:.\n",
      "\n",
      "CQI\n",
      "\n",
      "\n",
      "Prompt: 'The king'\n",
      "----------------------------------------\n",
      "The kingCCOBNtSt3AKSNLOSTXLKWJNNHNLDTLIGANCN:ENUNNRFGFZTOCOSNSSQCItSGSSCSSSOCSDQSSNTBSUUVALASONLSLISQTHFECCH3SSSGLKSLHNI::NKRNSCSrWCNGSR.NSDEOLMASNQ$RO?LT K\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-19T20:29:58.281240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train\n",
    "for iter in range(25000):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter:4d}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    optimizer.step(xb, yb)"
   ],
   "id": "87b36039a4c746b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0: train loss 2.3780, val loss 2.3950\n",
      "step  100: train loss 2.3778, val loss 2.3958\n",
      "step  200: train loss 2.3736, val loss 2.3937\n",
      "step  300: train loss 2.3712, val loss 2.3942\n",
      "step  400: train loss 2.3671, val loss 2.3887\n",
      "step  500: train loss 2.3627, val loss 2.3834\n",
      "step  600: train loss 2.3581, val loss 2.3799\n",
      "step  700: train loss 2.3584, val loss 2.3791\n",
      "step  800: train loss 2.3595, val loss 2.3740\n",
      "step  900: train loss 2.3505, val loss 2.3724\n",
      "step 1000: train loss 2.3535, val loss 2.3755\n",
      "step 1100: train loss 2.3457, val loss 2.3696\n",
      "step 1200: train loss 2.3475, val loss 2.3675\n",
      "step 1300: train loss 2.3401, val loss 2.3607\n",
      "step 1400: train loss 2.3371, val loss 2.3631\n",
      "step 1500: train loss 2.3353, val loss 2.3613\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate with priming\n",
    "def generate_with_priming(network, prompt=\"\", max_new_tokens=500, temperature=1.0):\n",
    "    prime_text = \"\"\"Act I. Scene I. Rome. A street.\n",
    "\n",
    "Enter a company of mutinous Citizens, with staves, clubs, and other weapons.\n",
    "\n",
    "First Citizen:\n",
    "Before we proceed any further, hear me speak.\n",
    "\n",
    "All:\n",
    "Speak, speak.\n",
    "\n",
    "First Citizen:\n",
    "You are all resolved rather to die than to famish?\n",
    "\n",
    "All:\n",
    "Resolved. Resolved.\n",
    "\n",
    "First Citizen:\n",
    "First, you know Caius Marcius is chief enemy to the people.\n",
    "\n",
    "All:\n",
    "We know't, we know't.\n",
    "\n",
    "First Citizen:\n",
    "Let us kill him, and we'll have corn at our own price.\n",
    "Is't a verdict?\n",
    "\n",
    "All:\n",
    "No more talking on't; let it be done: away, away!\n",
    "\n",
    "Second Citizen:\n",
    "One word, good citizens.\n",
    "\n",
    "First Citizen:\n",
    "We are accounted poor citizens, the patricians good.\n",
    "What authority surfeits on would relieve us: if they\n",
    "would yield us but the superfluity, while it were\n",
    "wholesome, we might guess they relieved us humanely; but\n",
    "they think we are too dear: the leanness that afflicts\n",
    "us, the object of our misery, is as an inventory to\n",
    "particularise their abundance; our sufferance is a\n",
    "gain to them Let us revenge this with our pikes, ere\n",
    "we become rakes: for the gods know I speak this in\n",
    "hunger for bread, not in thirst for revenge.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    full_start = prime_text + prompt\n",
    "    generated = generate_text(network, full_start, max_new_tokens, temperature)\n",
    "    return generated[len(prompt):]\n",
    "\n",
    "\n",
    "start_texts = [\n",
    "    \"First Citizen:\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"The \",\n",
    "]\n",
    "\n",
    "for start in start_texts:\n",
    "    generated = generate_with_priming(network, prompt=start)\n",
    "    print(f\"Starting with: {repr(start)}\")\n",
    "    print(generated)\n",
    "    print(\"-\" * 80)"
   ],
   "id": "2fe5c2c334922000",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
