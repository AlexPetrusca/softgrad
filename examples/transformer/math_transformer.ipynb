{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T07:47:40.971898Z",
     "start_time": "2025-12-22T07:47:40.916751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train a transformer to learn basic math operations on two number\n",
    "#   - Operators: add, subtract, multiply, divide, modulo\n",
    "\n",
    "import mlx.core as mx\n",
    "import numpy as np"
   ],
   "id": "17f062bbe3184f22",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T07:47:53.940483Z",
     "start_time": "2025-12-22T07:47:40.972469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset\n",
    "\n",
    "ADD = 101\n",
    "SUBTRACT = 102\n",
    "MULTIPLY = 103\n",
    "DIVIDE = 104\n",
    "\n",
    "max_num = 100\n",
    "operator_count = 5\n",
    "vocab_size = max_num + operator_count + 1\n",
    "\n",
    "def to_example(op):\n",
    "    if op == MULTIPLY:\n",
    "        x = mx.random.randint(1, max_num + 1)\n",
    "        y = mx.random.randint(0, max_num // x) # no overflow\n",
    "        return mx.array([x, op, y, x * y], dtype=mx.int32)\n",
    "    elif op == DIVIDE:\n",
    "        x = mx.random.randint(0, max_num + 1)\n",
    "        y = mx.random.randint(1, max_num + 1) # no division by 0\n",
    "        answer = x // y if op == DIVIDE else x % y\n",
    "        return mx.array([x, op, y, answer], dtype=mx.int32)\n",
    "    elif op == ADD:\n",
    "        x = mx.random.randint(0, max_num + 1)\n",
    "        y = mx.random.randint(0, max_num + 1 - x) # no overflow\n",
    "        return mx.array([x, op, y, x + y], dtype=mx.int32)\n",
    "    elif op == SUBTRACT:\n",
    "        x = mx.random.randint(0, max_num + 1)\n",
    "        y = mx.random.randint(0, x + 1) # no negative\n",
    "        return mx.array([x, op, y, x - y], dtype=mx.int32)\n",
    "\n",
    "ops_train = mx.random.randint(ADD, DIVIDE + 1, shape=(20000,))\n",
    "examples_train = mx.array([to_example(op) for op in ops_train])\n",
    "\n",
    "ops_val = mx.random.randint(ADD, DIVIDE + 1, shape=(2500,))\n",
    "examples_val = mx.array([to_example(op) for op in ops_val])\n",
    "\n",
    "def get_batch(split):\n",
    "    examples_split = examples_train if split == 'train' else examples_val\n",
    "    X_split = examples_split[:, :3]\n",
    "    Y_split = examples_split[:, 3:]\n",
    "\n",
    "    n = mx.random.randint(0, len(X_split) - block_size, (batch_size,))\n",
    "    return X_split[n], mx.stack([Y_split[n] for _ in range(X_split.shape[1])], axis=1)\n",
    "\n",
    "examples_val"
   ],
   "id": "f79ccf21ab74d69c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[58, 103, 0, 0],\n",
       "       [22, 101, 16, 38],\n",
       "       [65, 101, 4, 69],\n",
       "       ...,\n",
       "       [39, 101, 35, 74],\n",
       "       [82, 103, 0, 0],\n",
       "       [37, 103, 1, 37]], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-22T08:01:34.381811Z",
     "start_time": "2025-12-22T07:47:53.946649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Construct network\n",
    "\n",
    "from softgrad import Network\n",
    "from softgrad.function.activation import Relu\n",
    "from softgrad.function.core import Add, Concatenate\n",
    "from softgrad.function.loss import sequence_ce_loss\n",
    "from softgrad.layer.attn import CausalSelfAttention\n",
    "from softgrad.layer.core import Parallel, Embedding, Sequential, Linear, Residual, Activation\n",
    "from softgrad.layer.norm import LayerNorm\n",
    "from softgrad.layer.transform.PositionIndices import PositionIndices\n",
    "from softgrad.optim import AdamW\n",
    "\n",
    "\n",
    "class FeedForward(Sequential):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__([\n",
    "            Linear(4 * n_embd),\n",
    "            Activation(Relu()),\n",
    "            Linear(n_embd)\n",
    "        ])\n",
    "\n",
    "\n",
    "class MultiHeadAttention(Sequential):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__([\n",
    "            Parallel(\n",
    "                [CausalSelfAttention(n_embd, head_size, block_size) for _ in range(num_heads)]  # heads\n",
    "            , Concatenate()),\n",
    "            Linear(n_embd)  # projection\n",
    "        ])\n",
    "\n",
    "\n",
    "class TransformerBlock(Sequential):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__([\n",
    "            # communication\n",
    "            Residual(Sequential([\n",
    "                LayerNorm(),\n",
    "                MultiHeadAttention(n_head, n_embd // n_head)\n",
    "            ])),\n",
    "            # computation\n",
    "            Residual(Sequential([\n",
    "                LayerNorm(),\n",
    "                FeedForward(n_embd)\n",
    "            ]))\n",
    "        ])\n",
    "\n",
    "\n",
    "mx.random.seed(1337)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Hyperparameters\n",
    "# ----------------------------------------------------------------------------------\n",
    "batch_size = 128\n",
    "block_size = 3          # (num1, op, num2)\n",
    "max_iters = 5000\n",
    "eval_interval = 200\n",
    "learning_rate = 3e-2\n",
    "eval_iters = 50\n",
    "n_embd = 768             # each token -> 128\n",
    "n_head = 12              # 4 heads -> 32\n",
    "n_layer = 12             # 2 transformer blocks\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Setup Network\n",
    "# ----------------------------------------------------------------------------------\n",
    "network = Network(input_shape=(block_size,))\n",
    "network.add_layer(Parallel([\n",
    "    Embedding(vocab_size, n_embd),  # Semantic encoding\n",
    "    Sequential([\n",
    "        PositionIndices(),\n",
    "        Embedding(block_size, n_embd)  # Positional encoding\n",
    "    ])\n",
    "], Add()))\n",
    "network.add_layer(Sequential(\n",
    "    [TransformerBlock(n_embd, n_head) for _ in range(n_layer)]  # transformer blocks\n",
    "))\n",
    "network.add_layer(LayerNorm())\n",
    "network.add_layer(Linear(vocab_size))  # LLM head\n",
    "\n",
    "optimizer = AdamW(eta=3e-4, beta1=0.9, beta2=0.999, weight_decay=0.01)\n",
    "optimizer.bind_loss_fn(sequence_ce_loss)\n",
    "optimizer.bind_network(network)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Evaluation function\n",
    "# ----------------------------------------------------------------------------------\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = []\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "\n",
    "            # forward pass\n",
    "            logits = network.forward(X, save_ctx=False)\n",
    "\n",
    "            # compute loss\n",
    "            loss_per_token = sequence_ce_loss.apply(logits, Y)  # (10, 105, 4) -> expect (2, 2, 2)\n",
    "            mean_loss = mx.mean(loss_per_token)\n",
    "\n",
    "            losses.append(mean_loss.item())\n",
    "\n",
    "        out[split] = np.mean(losses)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Train Loop\n",
    "# ----------------------------------------------------------------------------------\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter:4d}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    optimizer.step(xb, yb)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Final Evaluation\n",
    "# ----------------------------------------------------------------------------------\n",
    "losses = estimate_loss()\n",
    "print(f\"Final: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
   ],
   "id": "bec04f11fae568fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0: train loss 5.0682, val loss 5.0585\n",
      "step  200: train loss 2.6535, val loss 2.6802\n",
      "step  400: train loss 2.0401, val loss 2.0784\n",
      "step  600: train loss 1.8001, val loss 1.8830\n",
      "step  800: train loss 1.5980, val loss 1.6190\n",
      "step 1000: train loss 1.4810, val loss 1.5123\n",
      "step 1200: train loss 1.4378, val loss 1.4878\n",
      "step 1400: train loss 1.5117, val loss 1.5636\n",
      "step 1600: train loss 1.4354, val loss 1.5528\n",
      "step 1800: train loss 1.2040, val loss 1.3054\n",
      "step 2000: train loss 1.1222, val loss 1.2183\n",
      "step 2200: train loss 1.0855, val loss 1.2046\n",
      "step 2400: train loss 1.1076, val loss 1.2230\n",
      "step 2600: train loss 1.0223, val loss 1.0989\n",
      "step 2800: train loss 1.0673, val loss 1.1521\n",
      "step 3000: train loss 0.8845, val loss 1.0024\n",
      "step 3200: train loss 0.9356, val loss 1.0634\n",
      "step 3400: train loss 0.9081, val loss 1.0336\n",
      "step 3600: train loss 0.8589, val loss 0.9373\n",
      "step 3800: train loss 0.9257, val loss 1.0278\n",
      "step 4000: train loss 0.7659, val loss 0.8894\n",
      "step 4200: train loss 1.0112, val loss 1.1048\n",
      "step 4400: train loss 0.8216, val loss 0.9693\n",
      "step 4600: train loss 0.7624, val loss 0.8747\n",
      "step 4800: train loss 0.7967, val loss 0.9070\n",
      "Final: train loss 0.7468, val loss 0.9133\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T18:12:20.557240Z",
     "start_time": "2025-12-22T17:57:52.324540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train some more\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter:4d}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    optimizer.step(xb, yb)"
   ],
   "id": "d2efacdb99d0377f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0: train loss 0.0179, val loss 0.1157\n",
      "step  200: train loss 0.0035, val loss 0.0895\n",
      "step  400: train loss 0.0015, val loss 0.0727\n",
      "step  600: train loss 0.0010, val loss 0.0896\n",
      "step  800: train loss 0.0007, val loss 0.0850\n",
      "step 1000: train loss 0.0006, val loss 0.0946\n",
      "step 1200: train loss 0.0005, val loss 0.0805\n",
      "step 1400: train loss 0.0004, val loss 0.0811\n",
      "step 1600: train loss 0.0003, val loss 0.0721\n",
      "step 1800: train loss 0.0003, val loss 0.0975\n",
      "step 2000: train loss 0.0003, val loss 0.0968\n",
      "step 2200: train loss 0.0003, val loss 0.0798\n",
      "step 2400: train loss 0.0002, val loss 0.0946\n",
      "step 2600: train loss 0.0002, val loss 0.0819\n",
      "step 2800: train loss 0.0002, val loss 0.0814\n",
      "step 3000: train loss 0.0002, val loss 0.0861\n",
      "step 3200: train loss 0.0002, val loss 0.0738\n",
      "step 3400: train loss 0.0001, val loss 0.0759\n",
      "step 3600: train loss 0.0001, val loss 0.0992\n",
      "step 3800: train loss 0.0001, val loss 0.1072\n",
      "step 4000: train loss 0.0001, val loss 0.0789\n",
      "step 4200: train loss 0.0001, val loss 0.0948\n",
      "step 4400: train loss 0.0001, val loss 0.0851\n",
      "step 4600: train loss 0.0001, val loss 0.0811\n",
      "step 4800: train loss 0.0001, val loss 0.0974\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T18:12:21.649061Z",
     "start_time": "2025-12-22T18:12:20.561110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for start in range(0, len(examples_val), batch_size):\n",
    "    end = start + batch_size\n",
    "    subbatch = examples_val[start:end]\n",
    "\n",
    "    X = subbatch[:, :3]\n",
    "    Y = subbatch[:, 3:]\n",
    "\n",
    "    logits = network.forward(X, save_ctx=False)\n",
    "    max_logits = mx.argmax(logits, axis=-1)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        if Y[i] == max_logits[i][0]:\n",
    "            correct += 1\n",
    "        # else:\n",
    "            # x, op, y = X\n",
    "            # print(f\"Error: {x} {op} {y} != {pred}\")\n",
    "\n",
    "    total += len(subbatch)\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ],
   "id": "c21256a170630986",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.32%\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
