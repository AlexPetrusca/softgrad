{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-21T22:34:05.114092Z",
     "start_time": "2025-12-21T22:32:03.775933Z"
    }
   },
   "source": [
    "# Train a transformer to learn the modulo operator\n",
    "\n",
    "import mlx.core as mx\n",
    "import numpy as np\n",
    "\n",
    "from softgrad import Network\n",
    "from softgrad.function.activation import Relu\n",
    "from softgrad.function.core import Add, Concatenate\n",
    "from softgrad.function.loss import sequence_ce_loss\n",
    "from softgrad.layer.attn import CausalSelfAttention\n",
    "from softgrad.layer.core import Parallel, Embedding, Sequential, Linear, Residual, Activation\n",
    "from softgrad.layer.norm import LayerNorm\n",
    "from softgrad.layer.transform.PositionIndices import PositionIndices\n",
    "from softgrad.optim import SGD\n",
    "\n",
    "\n",
    "class FeedForward(Sequential):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__([\n",
    "            Linear(4 * n_embd),\n",
    "            Activation(Relu()),\n",
    "            Linear(n_embd)\n",
    "        ])\n",
    "\n",
    "\n",
    "class MultiHeadAttention(Sequential):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__([\n",
    "            Parallel(\n",
    "                [CausalSelfAttention(n_embd, head_size, block_size) for _ in range(num_heads)]  # heads\n",
    "            , Concatenate()),\n",
    "            Linear(n_embd)  # projection\n",
    "        ])\n",
    "\n",
    "\n",
    "class TransformerBlock(Sequential):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__([\n",
    "            # communication\n",
    "            Residual(Sequential([\n",
    "                LayerNorm(),\n",
    "                MultiHeadAttention(n_head, n_embd // n_head)\n",
    "            ])),\n",
    "            # computation\n",
    "            Residual(Sequential([\n",
    "                LayerNorm(),\n",
    "                FeedForward(n_embd)\n",
    "            ]))\n",
    "        ])\n",
    "\n",
    "\n",
    "mx.random.seed(1337)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Hyperparameters\n",
    "# ----------------------------------------------------------------------------------\n",
    "batch_size = 32\n",
    "block_size = 2          # (num1, num2)\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-2\n",
    "eval_iters = 50\n",
    "n_embd = 128            # each token -> 128\n",
    "n_head = 4              # 4 heads -> 32\n",
    "n_layer = 2             # 2 transformer blocks\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Dataset\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "max_num = 100\n",
    "max_modulo = 20\n",
    "vocab_size = max_num + 1\n",
    "\n",
    "X_train = mx.random.randint(0, max_num + 1, shape=(10000,))\n",
    "Y_train = mx.random.randint(1, max_modulo + 1, shape=(10000,))\n",
    "Z_train = X_train % Y_train\n",
    "\n",
    "X_val = mx.random.randint(0, max_num + 1, shape=(1000,))\n",
    "Y_val = mx.random.randint(1, max_modulo + 1, shape=(1000,))\n",
    "Z_val = X_val % Y_val\n",
    "\n",
    "def get_batch(split):\n",
    "    X_split = X_train if split == 'train' else X_val\n",
    "    Y_split = Y_train if split == 'train' else Y_val\n",
    "    Z_split = Z_train if split == 'train' else Z_val\n",
    "\n",
    "    XY_split = mx.stack([X_split, Y_split], -1)\n",
    "    n = mx.random.randint(0, len(XY_split) - block_size, (batch_size,))\n",
    "    return XY_split[n], mx.stack([Z_split[n], Z_split[n]], axis=1)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Setup Network\n",
    "# ----------------------------------------------------------------------------------\n",
    "network = Network(input_shape=(block_size,))\n",
    "network.add_layer(Parallel([\n",
    "    Embedding(vocab_size, n_embd),  # Semantic encoding\n",
    "    Sequential([\n",
    "        PositionIndices(),\n",
    "        Embedding(block_size, n_embd)  # Positional encoding\n",
    "    ])\n",
    "], Add()))\n",
    "network.add_layer(Sequential(\n",
    "    [TransformerBlock(n_embd, n_head) for _ in range(n_layer)]  # transformer blocks\n",
    "))\n",
    "network.add_layer(LayerNorm())\n",
    "network.add_layer(Linear(vocab_size))  # LLM head\n",
    "\n",
    "optimizer = SGD(eta=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer.bind_loss_fn(sequence_ce_loss)\n",
    "optimizer.bind_network(network)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Evaluation function\n",
    "# ----------------------------------------------------------------------------------\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = []\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "\n",
    "            # forward pass\n",
    "            logits = network.forward(X, save_ctx=False)\n",
    "\n",
    "            # compute loss\n",
    "            loss_per_token = sequence_ce_loss.apply(logits, Y)  # (10, 4) -> expect (2, 2)\n",
    "            mean_loss = mx.mean(loss_per_token)\n",
    "\n",
    "            losses.append(mean_loss.item())\n",
    "\n",
    "        out[split] = np.mean(losses)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Train Loop\n",
    "# ----------------------------------------------------------------------------------\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter:4d}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    optimizer.step(xb, yb)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Final Evaluation\n",
    "# ----------------------------------------------------------------------------------\n",
    "losses = estimate_loss()\n",
    "print(f\"Final: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0: train loss 4.6237, val loss 4.6260\n",
      "step  100: train loss 2.7067, val loss 2.6777\n",
      "step  200: train loss 2.4880, val loss 2.5322\n",
      "step  300: train loss 2.2969, val loss 2.3370\n",
      "step  400: train loss 2.1288, val loss 2.1898\n",
      "step  500: train loss 1.9673, val loss 2.0614\n",
      "step  600: train loss 1.9223, val loss 2.0451\n",
      "step  700: train loss 1.8187, val loss 1.9607\n",
      "step  800: train loss 1.7907, val loss 1.9172\n",
      "step  900: train loss 1.6755, val loss 1.8464\n",
      "step 1000: train loss 1.6625, val loss 1.8917\n",
      "step 1100: train loss 1.6773, val loss 1.7899\n",
      "step 1200: train loss 1.6784, val loss 1.7598\n",
      "step 1300: train loss 1.7026, val loss 1.7837\n",
      "step 1400: train loss 1.5759, val loss 1.7471\n",
      "step 1500: train loss 1.5897, val loss 1.8180\n",
      "step 1600: train loss 1.5529, val loss 1.6558\n",
      "step 1700: train loss 1.4291, val loss 1.6648\n",
      "step 1800: train loss 1.4499, val loss 1.6060\n",
      "step 1900: train loss 1.4011, val loss 1.5371\n",
      "step 2000: train loss 1.4102, val loss 1.5616\n",
      "step 2100: train loss 1.3124, val loss 1.5514\n",
      "step 2200: train loss 1.3549, val loss 1.5529\n",
      "step 2300: train loss 1.3262, val loss 1.5076\n",
      "step 2400: train loss 1.2978, val loss 1.5207\n",
      "step 2500: train loss 1.2593, val loss 1.4385\n",
      "step 2600: train loss 1.2880, val loss 1.3938\n",
      "step 2700: train loss 1.1612, val loss 1.3126\n",
      "step 2800: train loss 1.0976, val loss 1.2836\n",
      "step 2900: train loss 1.0638, val loss 1.2626\n",
      "step 3000: train loss 1.0780, val loss 1.2465\n",
      "step 3100: train loss 0.9965, val loss 1.1797\n",
      "step 3200: train loss 0.9438, val loss 1.1497\n",
      "step 3300: train loss 0.9363, val loss 1.0623\n",
      "step 3400: train loss 0.8628, val loss 1.0335\n",
      "step 3500: train loss 0.7969, val loss 1.0527\n",
      "step 3600: train loss 0.6904, val loss 0.8634\n",
      "step 3700: train loss 0.7588, val loss 0.9484\n",
      "step 3800: train loss 0.6592, val loss 0.7939\n",
      "step 3900: train loss 0.6384, val loss 0.7741\n",
      "step 4000: train loss 0.5839, val loss 0.7021\n",
      "step 4100: train loss 0.5684, val loss 0.7178\n",
      "step 4200: train loss 0.5552, val loss 0.6977\n",
      "step 4300: train loss 0.4242, val loss 0.6043\n",
      "step 4400: train loss 0.4296, val loss 0.5892\n",
      "step 4500: train loss 0.3762, val loss 0.6347\n",
      "step 4600: train loss 0.3807, val loss 0.5504\n",
      "step 4700: train loss 0.3606, val loss 0.4290\n",
      "step 4800: train loss 0.3441, val loss 0.5386\n",
      "step 4900: train loss 0.2862, val loss 0.4524\n",
      "Final: train loss 0.2585, val loss 0.4481\n",
      "array([[[10.8586, 4.82724, 12.0315, ..., 0.0584944, -1.6789, 1.01514],\n",
      "        [7.75728, 3.8387, 12.2012, ..., 0.183027, -1.69698, 1.14994]]], dtype=float32)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T22:38:35.213427Z",
     "start_time": "2025-12-21T22:36:34.532875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train some more\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter:4d}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    optimizer.step(xb, yb)"
   ],
   "id": "66ee5933b7618889",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step    0: train loss 0.2745, val loss 0.4082\n",
      "step  100: train loss 0.2337, val loss 0.3658\n",
      "step  200: train loss 0.2284, val loss 0.3249\n",
      "step  300: train loss 0.2084, val loss 0.3677\n",
      "step  400: train loss 0.2171, val loss 0.3088\n",
      "step  500: train loss 0.1838, val loss 0.3319\n",
      "step  600: train loss 0.1557, val loss 0.2726\n",
      "step  700: train loss 0.1356, val loss 0.2482\n",
      "step  800: train loss 0.1274, val loss 0.2277\n",
      "step  900: train loss 0.1075, val loss 0.2188\n",
      "step 1000: train loss 0.1088, val loss 0.2127\n",
      "step 1100: train loss 0.0913, val loss 0.1934\n",
      "step 1200: train loss 0.0868, val loss 0.1273\n",
      "step 1300: train loss 0.0895, val loss 0.2176\n",
      "step 1400: train loss 0.0638, val loss 0.1576\n",
      "step 1500: train loss 0.0526, val loss 0.1477\n",
      "step 1600: train loss 0.0412, val loss 0.1590\n",
      "step 1700: train loss 0.0390, val loss 0.1101\n",
      "step 1800: train loss 0.0268, val loss 0.1292\n",
      "step 1900: train loss 0.0352, val loss 0.1300\n",
      "step 2000: train loss 0.0288, val loss 0.1003\n",
      "step 2100: train loss 0.0264, val loss 0.0726\n",
      "step 2200: train loss 0.0206, val loss 0.0909\n",
      "step 2300: train loss 0.0171, val loss 0.0796\n",
      "step 2400: train loss 0.0147, val loss 0.0851\n",
      "step 2500: train loss 0.0173, val loss 0.1430\n",
      "step 2600: train loss 0.0142, val loss 0.0923\n",
      "step 2700: train loss 0.0105, val loss 0.0928\n",
      "step 2800: train loss 0.0111, val loss 0.0654\n",
      "step 2900: train loss 0.0112, val loss 0.0509\n",
      "step 3000: train loss 0.0111, val loss 0.0866\n",
      "step 3100: train loss 0.0101, val loss 0.0878\n",
      "step 3200: train loss 0.0097, val loss 0.0485\n",
      "step 3300: train loss 0.0085, val loss 0.0817\n",
      "step 3400: train loss 0.0086, val loss 0.0696\n",
      "step 3500: train loss 0.0074, val loss 0.0460\n",
      "step 3600: train loss 0.0076, val loss 0.0794\n",
      "step 3700: train loss 0.0076, val loss 0.0719\n",
      "step 3800: train loss 0.0072, val loss 0.0595\n",
      "step 3900: train loss 0.0074, val loss 0.0858\n",
      "step 4000: train loss 0.0063, val loss 0.1040\n",
      "step 4100: train loss 0.0067, val loss 0.0770\n",
      "step 4200: train loss 0.0060, val loss 0.0685\n",
      "step 4300: train loss 0.0070, val loss 0.0816\n",
      "step 4400: train loss 0.0055, val loss 0.0512\n",
      "step 4500: train loss 0.0056, val loss 0.0831\n",
      "step 4600: train loss 0.0058, val loss 0.0808\n",
      "step 4700: train loss 0.0053, val loss 0.0709\n",
      "step 4800: train loss 0.0053, val loss 0.1205\n",
      "step 4900: train loss 0.0047, val loss 0.0949\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-21T22:49:43.285153Z",
     "start_time": "2025-12-21T22:49:40.798615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# evaluate\n",
    "total = 0\n",
    "correct = 0\n",
    "for x in range(max_num + 1):\n",
    "    for y in range(1, max_modulo + 1):\n",
    "        logits = network.forward(mx.array([[x, y]], dtype=mx.int32), save_ctx=False)\n",
    "        max_logit = mx.argmax(logits, axis=-1)[0]\n",
    "\n",
    "        pred = max_logit[0]\n",
    "        if (x % y) == int(pred):\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(f\"Error: {x} % {y} = {x % y}, not {pred}\")\n",
    "\n",
    "        total += 1\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ],
   "id": "6ba2c7b346a0d34f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 15 % 12 = 3, not 7\n",
      "Error: 35 % 10 = 5, not 7\n",
      "Error: 57 % 7 = 1, not 2\n",
      "Error: 60 % 12 = 0, not 6\n",
      "Error: 64 % 13 = 12, not 13\n",
      "Error: 64 % 18 = 10, not 4\n",
      "Error: 67 % 7 = 4, not 2\n",
      "Error: 81 % 18 = 9, not 6\n",
      "Error: 84 % 16 = 4, not 12\n",
      "Error: 94 % 19 = 18, not 9\n",
      "Error: 95 % 11 = 7, not 4\n",
      "Error: 96 % 7 = 5, not 1\n",
      "Error: 99 % 13 = 8, not 0\n",
      "Accuracy: 99.36%\n"
     ]
    }
   ],
   "execution_count": 70
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
